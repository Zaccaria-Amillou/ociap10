{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projet 10: Application de recommandation de contenu\n",
    "### Partie 2: Modèlisation \n",
    "Dans ce notebook on vas procéder à effectuer la modélisation des nos données pour mettre en place une systeme de recommandation du contenu.\n",
    "Nous allons utiliser deux approches principales:\n",
    "- Content-based filtering\n",
    "- Collaborative-based filtering\n",
    "\n",
    "Nous allons nous appuyer sur la libraries [Surpise](https://surpriselib.com/) pour mettre en place nos modèles "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Import des libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from math import floor\n",
    "import numpy as np\n",
    "\n",
    "import surprise\n",
    "from surprise import SVD, Dataset, Reader, KNNBasic, KNNWithMeans\n",
    "from surprise.model_selection import GridSearchCV\n",
    "from surprise.model_selection import train_test_split\n",
    "from heapq import nlargest\n",
    "\n",
    "from surprise import accuracy\n",
    "from surprise import Dataset, Reader\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import SVDpp\n",
    "from collections import defaultdict\n",
    "from heapq import nlargest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Import des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant nous allons procéder aves l'importations des libraries que nous allons utiliser pour la modèlisation et puis on vas importer les fichiers des données que nous allons utiliser comme base pour les modèles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 Définition des chemins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/raw/globocom/\"\n",
    "clicks_path= \"../data/raw/globocom/clicks/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 Import fichier avec metadonnées des articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>category_id</th>\n",
       "      <th>publisher_id</th>\n",
       "      <th>words_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>162</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id  category_id  publisher_id  words_count\n",
       "0           0            0             0          168\n",
       "1           1            1             0          189\n",
       "2           2            1             0          250\n",
       "3           3            1             0          230\n",
       "4           4            1             0          162"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_df = pd.read_csv(data_path + 'articles_metadata.csv')\n",
    "articles_df.drop(columns=['created_at_ts'], inplace=True)\n",
    "articles_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['article_id', 'category_id', 'publisher_id', 'words_count'], dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3 Import fichiers avec embeddings des articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>240</th>\n",
       "      <th>241</th>\n",
       "      <th>242</th>\n",
       "      <th>243</th>\n",
       "      <th>244</th>\n",
       "      <th>245</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.161183</td>\n",
       "      <td>-0.957233</td>\n",
       "      <td>-0.137944</td>\n",
       "      <td>0.050855</td>\n",
       "      <td>0.830055</td>\n",
       "      <td>0.901365</td>\n",
       "      <td>-0.335148</td>\n",
       "      <td>-0.559561</td>\n",
       "      <td>-0.500603</td>\n",
       "      <td>0.165183</td>\n",
       "      <td>...</td>\n",
       "      <td>0.321248</td>\n",
       "      <td>0.313999</td>\n",
       "      <td>0.636412</td>\n",
       "      <td>0.169179</td>\n",
       "      <td>0.540524</td>\n",
       "      <td>-0.813182</td>\n",
       "      <td>0.286870</td>\n",
       "      <td>-0.231686</td>\n",
       "      <td>0.597416</td>\n",
       "      <td>0.409623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.523216</td>\n",
       "      <td>-0.974058</td>\n",
       "      <td>0.738608</td>\n",
       "      <td>0.155234</td>\n",
       "      <td>0.626294</td>\n",
       "      <td>0.485297</td>\n",
       "      <td>-0.715657</td>\n",
       "      <td>-0.897996</td>\n",
       "      <td>-0.359747</td>\n",
       "      <td>0.398246</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.487843</td>\n",
       "      <td>0.823124</td>\n",
       "      <td>0.412688</td>\n",
       "      <td>-0.338654</td>\n",
       "      <td>0.320787</td>\n",
       "      <td>0.588643</td>\n",
       "      <td>-0.594137</td>\n",
       "      <td>0.182828</td>\n",
       "      <td>0.397090</td>\n",
       "      <td>-0.834364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.619619</td>\n",
       "      <td>-0.972960</td>\n",
       "      <td>-0.207360</td>\n",
       "      <td>-0.128861</td>\n",
       "      <td>0.044748</td>\n",
       "      <td>-0.387535</td>\n",
       "      <td>-0.730477</td>\n",
       "      <td>-0.066126</td>\n",
       "      <td>-0.754899</td>\n",
       "      <td>-0.242004</td>\n",
       "      <td>...</td>\n",
       "      <td>0.454756</td>\n",
       "      <td>0.473184</td>\n",
       "      <td>0.377866</td>\n",
       "      <td>-0.863887</td>\n",
       "      <td>-0.383365</td>\n",
       "      <td>0.137721</td>\n",
       "      <td>-0.810877</td>\n",
       "      <td>-0.447580</td>\n",
       "      <td>0.805932</td>\n",
       "      <td>-0.285284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.740843</td>\n",
       "      <td>-0.975749</td>\n",
       "      <td>0.391698</td>\n",
       "      <td>0.641738</td>\n",
       "      <td>-0.268645</td>\n",
       "      <td>0.191745</td>\n",
       "      <td>-0.825593</td>\n",
       "      <td>-0.710591</td>\n",
       "      <td>-0.040099</td>\n",
       "      <td>-0.110514</td>\n",
       "      <td>...</td>\n",
       "      <td>0.271535</td>\n",
       "      <td>0.036040</td>\n",
       "      <td>0.480029</td>\n",
       "      <td>-0.763173</td>\n",
       "      <td>0.022627</td>\n",
       "      <td>0.565165</td>\n",
       "      <td>-0.910286</td>\n",
       "      <td>-0.537838</td>\n",
       "      <td>0.243541</td>\n",
       "      <td>-0.885329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.279052</td>\n",
       "      <td>-0.972315</td>\n",
       "      <td>0.685374</td>\n",
       "      <td>0.113056</td>\n",
       "      <td>0.238315</td>\n",
       "      <td>0.271913</td>\n",
       "      <td>-0.568816</td>\n",
       "      <td>0.341194</td>\n",
       "      <td>-0.600554</td>\n",
       "      <td>-0.125644</td>\n",
       "      <td>...</td>\n",
       "      <td>0.238286</td>\n",
       "      <td>0.809268</td>\n",
       "      <td>0.427521</td>\n",
       "      <td>-0.615932</td>\n",
       "      <td>-0.503697</td>\n",
       "      <td>0.614450</td>\n",
       "      <td>-0.917760</td>\n",
       "      <td>-0.424061</td>\n",
       "      <td>0.185484</td>\n",
       "      <td>-0.580292</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 250 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0 -0.161183 -0.957233 -0.137944  0.050855  0.830055  0.901365 -0.335148   \n",
       "1 -0.523216 -0.974058  0.738608  0.155234  0.626294  0.485297 -0.715657   \n",
       "2 -0.619619 -0.972960 -0.207360 -0.128861  0.044748 -0.387535 -0.730477   \n",
       "3 -0.740843 -0.975749  0.391698  0.641738 -0.268645  0.191745 -0.825593   \n",
       "4 -0.279052 -0.972315  0.685374  0.113056  0.238315  0.271913 -0.568816   \n",
       "\n",
       "        7         8         9    ...       240       241       242       243  \\\n",
       "0 -0.559561 -0.500603  0.165183  ...  0.321248  0.313999  0.636412  0.169179   \n",
       "1 -0.897996 -0.359747  0.398246  ... -0.487843  0.823124  0.412688 -0.338654   \n",
       "2 -0.066126 -0.754899 -0.242004  ...  0.454756  0.473184  0.377866 -0.863887   \n",
       "3 -0.710591 -0.040099 -0.110514  ...  0.271535  0.036040  0.480029 -0.763173   \n",
       "4  0.341194 -0.600554 -0.125644  ...  0.238286  0.809268  0.427521 -0.615932   \n",
       "\n",
       "        244       245       246       247       248       249  \n",
       "0  0.540524 -0.813182  0.286870 -0.231686  0.597416  0.409623  \n",
       "1  0.320787  0.588643 -0.594137  0.182828  0.397090 -0.834364  \n",
       "2 -0.383365  0.137721 -0.810877 -0.447580  0.805932 -0.285284  \n",
       "3  0.022627  0.565165 -0.910286 -0.537838  0.243541 -0.885329  \n",
       "4 -0.503697  0.614450 -0.917760 -0.424061  0.185484 -0.580292  \n",
       "\n",
       "[5 rows x 250 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ouvrir le fichiers pickle et afficher les 5 premiers lignes\n",
    "with open(data_path + 'articles_embeddings.pickle', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "embeddings_df = pd.DataFrame(data)\n",
    "embeddings_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.4 Import fichier avec les interactions des utilisateurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_files_clicks(path):\n",
    "    clicks_df = pd.DataFrame()\n",
    "    for file in os.listdir(path):\n",
    "        df = pd.read_csv(path + file)\n",
    "        clicks_df = pd.concat([clicks_df, df], axis=0)\n",
    "\n",
    "    return clicks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "clicks_df = get_all_files_clicks(clicks_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>session_id</th>\n",
       "      <th>session_start</th>\n",
       "      <th>session_size</th>\n",
       "      <th>click_article_id</th>\n",
       "      <th>click_timestamp</th>\n",
       "      <th>click_environment</th>\n",
       "      <th>click_deviceGroup</th>\n",
       "      <th>click_os</th>\n",
       "      <th>click_country</th>\n",
       "      <th>click_region</th>\n",
       "      <th>click_referrer_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>93863</td>\n",
       "      <td>1507865792177843</td>\n",
       "      <td>2017-10-13 03:36:32</td>\n",
       "      <td>2</td>\n",
       "      <td>96210</td>\n",
       "      <td>2017-10-13 03:37:12.925</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>93863</td>\n",
       "      <td>1507865792177843</td>\n",
       "      <td>2017-10-13 03:36:32</td>\n",
       "      <td>2</td>\n",
       "      <td>158094</td>\n",
       "      <td>2017-10-13 03:37:42.925</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>294036</td>\n",
       "      <td>1507865795185844</td>\n",
       "      <td>2017-10-13 03:36:35</td>\n",
       "      <td>2</td>\n",
       "      <td>20691</td>\n",
       "      <td>2017-10-13 03:36:59.095</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>294036</td>\n",
       "      <td>1507865795185844</td>\n",
       "      <td>2017-10-13 03:36:35</td>\n",
       "      <td>2</td>\n",
       "      <td>96210</td>\n",
       "      <td>2017-10-13 03:37:29.095</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>77136</td>\n",
       "      <td>1507865796257845</td>\n",
       "      <td>2017-10-13 03:36:36</td>\n",
       "      <td>2</td>\n",
       "      <td>336245</td>\n",
       "      <td>2017-10-13 03:42:13.178</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_id        session_id       session_start session_size click_article_id  \\\n",
       "0   93863  1507865792177843 2017-10-13 03:36:32            2            96210   \n",
       "1   93863  1507865792177843 2017-10-13 03:36:32            2           158094   \n",
       "2  294036  1507865795185844 2017-10-13 03:36:35            2            20691   \n",
       "3  294036  1507865795185844 2017-10-13 03:36:35            2            96210   \n",
       "4   77136  1507865796257845 2017-10-13 03:36:36            2           336245   \n",
       "\n",
       "          click_timestamp click_environment click_deviceGroup click_os  \\\n",
       "0 2017-10-13 03:37:12.925                 4                 3        2   \n",
       "1 2017-10-13 03:37:42.925                 4                 3        2   \n",
       "2 2017-10-13 03:36:59.095                 4                 3       20   \n",
       "3 2017-10-13 03:37:29.095                 4                 3       20   \n",
       "4 2017-10-13 03:42:13.178                 4                 3        2   \n",
       "\n",
       "  click_country click_region click_referrer_type  \n",
       "0             1           21                   2  \n",
       "1             1           21                   2  \n",
       "2             1            9                   2  \n",
       "3             1            9                   2  \n",
       "4             1           25                   2  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clicks_df['click_timestamp'] = pd.to_datetime(clicks_df['click_timestamp'], unit='ms')\n",
    "clicks_df['session_start'] = pd.to_datetime(clicks_df['session_start'], unit='ms')\n",
    "clicks_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Content-based Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le **Content-based Filtering** est une méthode de recommandation qui utilise des informations détaillées sur les éléments pour recommander d'autres éléments similaires. Par exemple, dans un système de recommandation de films, le filtrage basé sur le contenu pourrait utiliser des informations telles que le genre du film, le réalisateur, les acteurs, etc.\n",
    "\n",
    "**Principe**\n",
    "L'idée est que si un utilisateur a aimé un certain élément dans le passé, il est probable qu'il aimera à nouveau des éléments similaires à l'avenir. Par conséquent, le système recommande des éléments qui sont similaires aux éléments que l'utilisateur a aimés précédemment.\n",
    "\n",
    "**Calcul de la similarité**\n",
    "La similarité entre les éléments est généralement calculée en utilisant des techniques telles que la similarité cosinus ou la distance euclidienne. Les éléments qui sont les plus similaires à ceux que l'utilisateur a aimés sont recommandés.\n",
    "\n",
    "**Note importante**\n",
    "Il est important de noter que le filtrage basé sur le contenu ne tient pas compte des opinions d'autres utilisateurs. Il se concentre uniquement sur les préférences de l'utilisateur actuel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_articles(articles, clicks, user_id, n=5):\n",
    "    # Convert user_id and click_article_id to integer type\n",
    "    clicks['user_id'] = clicks['user_id'].astype(int)\n",
    "    clicks['click_article_id'] = clicks['click_article_id'].astype(int)\n",
    "    articles.index = articles.index.astype(int)\n",
    "    \n",
    "    # Get the articles read by the user\n",
    "    articles_read = clicks[clicks['user_id'] == int(user_id)]['click_article_id'].tolist()\n",
    "    print(f\"Articles read by user {user_id}: {articles_read}\")\n",
    "\n",
    "    # If the user hasn't read any articles, recommend the most popular ones\n",
    "    if len(articles_read) == 0:\n",
    "        most_popular_articles = clicks['click_article_id'].value_counts().index.tolist()\n",
    "        print(f\"User {user_id} has not read any articles. Recommending most popular articles: {most_popular_articles[:n]}\")\n",
    "        return most_popular_articles[:n]\n",
    "\n",
    "    # Get the embeddings of the articles read by the user\n",
    "    articles_read_embedding = articles.loc[articles_read]\n",
    "    print(f\"Number of articles read by user {user_id}: {len(articles_read)}\")\n",
    "\n",
    "    # Remove the articles read by the user from the list of articles\n",
    "    articles = articles.drop(articles_read)\n",
    "    print(f\"Remaining articles after removing articles read by user {user_id}: {len(articles)}\")\n",
    "\n",
    "    # Calculate the cosine similarity between the articles read by the user and the other articles\n",
    "    matrix = cosine_similarity(articles_read_embedding, articles)\n",
    "\n",
    "    recommendations = []\n",
    "\n",
    "    # Recommend the articles most similar to the articles read by the user\n",
    "    for i in range(n):\n",
    "        coord_x = floor(np.argmax(matrix)/matrix.shape[1])\n",
    "        coord_y = np.argmax(matrix)%matrix.shape[1]\n",
    "\n",
    "        recommendations.append(int(articles.index[coord_y]))\n",
    "\n",
    "        # Set the similarity of the recommended article to 0\n",
    "        matrix[coord_x][coord_y] = 0\n",
    "\n",
    "    # Print the number of recommended articles that have already been read by the user\n",
    "    already_read = len(set(recommendations) & set(articles_read))\n",
    "    print(f\"Number of recommended articles that have already been read by user {user_id}: {already_read}\")\n",
    "\n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = 7723"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Articles read by user 7723: [214455, 9308, 9649, 129799, 141548, 336220, 353673, 337192, 84763, 107179, 199197, 271400, 84835, 338339, 60252, 303565, 31520, 36685, 36609, 163505, 123434, 141050, 313504, 272660, 72618, 72646, 140445, 277491, 226648, 57740, 128551, 140324, 198659, 166581, 156560, 282964, 225124, 277491, 128707]\n",
      "Number of articles read by user 7723: 39\n",
      "Remaining articles after removing articles read by user 7723: 364009\n",
      "Number of recommended articles that have already been read by user 7723: 0\n",
      "recommended articles: [336221, 226650, 303553, 226639, 129806, 287088, 236086, 336171, 156544, 271364]\n"
     ]
    }
   ],
   "source": [
    "# Assuming `articles_df` is your articles data and `user_id` is the id of the user you want to evaluate\n",
    "recommend = recommend_articles(articles_df, clicks_df, user_id, 10)\n",
    "print(f\"recommended articles: {recommend}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Collaborative-based Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le **Collaborative-based Filtering** est une méthode de recommandation qui se base sur les comportements passés des utilisateurs pour faire des prédictions sur ce qu'un utilisateur pourrait aimer.\n",
    "\n",
    "**Principe**\n",
    "L'idée principale est que si deux utilisateurs ont eu des comportements similaires par le passé (par exemple, ils ont aimé les mêmes films ou acheté les mêmes produits), alors ils sont susceptibles d'avoir des intérêts similaires à l'avenir.\n",
    "\n",
    "**Types de Filtrage Collaboratif**\n",
    "Il existe deux types principaux de filtrage collaboratif :\n",
    "\n",
    "1. **Filtrage Collaboratif Basé sur les Utilisateurs** : Cette méthode trouve des utilisateurs similaires à l'utilisateur cible et recommande des éléments que ces utilisateurs similaires ont aimés.\n",
    "\n",
    "2. **Filtrage Collaboratif Basé sur les Éléments** : Cette méthode trouve des éléments similaires à ceux que l'utilisateur cible a aimés et recommande ces éléments similaires.\n",
    "\n",
    "3. **Filtrage Collaboratif Basé sur un Modèle** : Cette méthode utilise des techniques de modélisation, comme la factorisation de matrices ou le clustering, pour prédire l'intérêt d'un utilisateur pour un élément. Elle se base sur les comportements passés de tous les utilisateurs, ainsi que sur les évaluations que l'utilisateur cible a données à d'autres éléments.\n",
    "\n",
    "**Calcul de la similarité**\n",
    "La similarité entre les utilisateurs ou les éléments est généralement calculée en utilisant des techniques telles que la corrélation de Pearson ou la similarité cosinus.\n",
    "\n",
    "**Note importante**\n",
    "Contrairement au filtrage basé sur le contenu, le filtrage collaboratif ne nécessite pas d'informations détaillées sur les éléments. Il se base uniquement sur les interactions passées entre les utilisateurs et les éléments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tout d'abord, nous allons rechercher les meilleurs paramètres pour le modèle en utilisant `GridSearchCV`. `GridSearchCV` est une méthode de recherche exhaustive qui parcourt toutes les combinaisons possibles de paramètres pour trouver celle qui produit le meilleur score de validation croisée.\n",
    "\n",
    "Dans le contexte de l'apprentissage automatique, les paramètres sont les configurations du modèle que nous ajustons pour améliorer la performance. Par exemple, dans un modèle de forêt aléatoire, les paramètres pourraient inclure le nombre d'arbres dans la forêt (`n_estimators`) et la profondeur maximale des arbres (`max_depth`).\n",
    "\n",
    "`GridSearchCV` fonctionne en entraînant et en évaluant un modèle pour chaque combinaison de paramètres. Il utilise la validation croisée pour évaluer la performance du modèle, ce qui signifie qu'il divise les données en un ensemble d'entraînement et un ensemble de test, entraîne le modèle sur l'ensemble d'entraînement, puis évalue la performance sur l'ensemble de test.\n",
    "\n",
    "Une fois que `GridSearchCV` a terminé la recherche, nous pouvons obtenir les meilleurs paramètres en utilisant l'attribut `best_params_`. Nous pouvons ensuite utiliser ces paramètres pour entraîner notre modèle final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for SVD: {'n_epochs': 10, 'lr_all': 0.005, 'reg_all': 0.4}\n",
      "Best RMSE for SVD: 0.28798814019027646\n",
      "Best MAE for SVD: 0.05711371978412593\n",
      "Best parameters for SVDpp: {'n_epochs': 10, 'lr_all': 0.005, 'reg_all': 0.4}\n",
      "Best RMSE for SVDpp: 0.28899137485788406\n",
      "Best MAE for SVDpp: 0.054097179295432146\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Best parameters for KNNWithMeans: {'k': 20, 'sim_options': {'name': 'msd', 'user_based': False}}\n",
      "Best RMSE for KNNWithMeans: 0.3053635006599857\n",
      "Best MAE for KNNWithMeans: 0.06530060814579225\n",
      "Best parameters for CoClustering: {'n_cltr_u': 3, 'n_cltr_i': 5}\n",
      "Best RMSE for CoClustering: 0.3414374214364575\n",
      "Best MAE for CoClustering: 0.07806218725918639\n",
      "Best parameters for SlopeOne: {}\n",
      "Best RMSE for SlopeOne: 0.325645474914508\n",
      "Best MAE for SlopeOne: 0.08349550453485181\n",
      "Best parameters for NormalPredictor: {}\n",
      "Best RMSE for NormalPredictor: 0.44801332667996424\n",
      "Best MAE for NormalPredictor: 0.2771092882007739\n",
      "\n",
      "Best model: SVD\n",
      "Best parameters: {'n_epochs': 10, 'lr_all': 0.005, 'reg_all': 0.4}\n",
      "Best RMSE: 0.28798814019027646\n"
     ]
    }
   ],
   "source": [
    "from surprise import SVD, SVDpp, KNNWithMeans, CoClustering, SlopeOne, NormalPredictor\n",
    "from surprise.model_selection import GridSearchCV\n",
    "\n",
    "# Create a 'click_count' column\n",
    "clicks_df['click_count'] = clicks_df.groupby(['user_id', 'click_article_id'])['click_timestamp'].transform('count')\n",
    "\n",
    "# Load a fraction of the data into a Surprise dataset\n",
    "reader = Reader(rating_scale=(0, clicks_df.click_count.max()))\n",
    "data = Dataset.load_from_df(clicks_df[['user_id', 'click_article_id', 'click_count']].sample(frac=0.1, random_state=42), reader)\n",
    "\n",
    "\n",
    "models = {\n",
    "    SVD: {'n_epochs': [5, 10], 'lr_all': [0.002, 0.005], 'reg_all': [0.4, 0.6]},\n",
    "    SVDpp: {'n_epochs': [5, 10], 'lr_all': [0.002, 0.005], 'reg_all': [0.4, 0.6]},\n",
    "    KNNWithMeans: {'k': [20], 'sim_options': {'name': ['msd', 'cosine'], 'user_based': [False]}},\n",
    "    CoClustering: {'n_cltr_u': [3, 5], 'n_cltr_i': [3, 5]},\n",
    "    SlopeOne: {},\n",
    "    NormalPredictor: {}\n",
    "}\n",
    "\n",
    "best_score = float('inf')\n",
    "best_model = None\n",
    "best_params = None\n",
    "\n",
    "for model, param_grid in models.items():\n",
    "    gs = GridSearchCV(model, param_grid, measures=['rmse', 'mae'], cv=3)\n",
    "    gs.fit(data)\n",
    "    params = gs.best_params['rmse']\n",
    "    score = gs.best_score['rmse']\n",
    "    print(f\"Best parameters for {model.__name__}: {params}\")\n",
    "    print(f\"Best RMSE for {model.__name__}: {score}\")\n",
    "    print(f\"Best MAE for {model.__name__}: {gs.best_score['mae']}\")\n",
    "    if score < best_score:\n",
    "        best_score = score\n",
    "        best_model = model.__name__\n",
    "        best_params = params\n",
    "\n",
    "print(f\"\\nBest model: {best_model}\")\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "print(f\"Best RMSE: {best_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'n_factors': 20, 'n_epochs': 10, 'lr_all': 0.002, 'reg_all': 0.01}\n",
      "Best RMSE: 0.19410449113534123\n",
      "Best MAE: 0.04801835921096572\n"
     ]
    }
   ],
   "source": [
    "# Create a 'click_count' column\n",
    "clicks_df['click_count'] = clicks_df.groupby(['user_id', 'click_article_id'])['click_timestamp'].transform('count')\n",
    "\n",
    "# Load a fraction of the data into a Surprise dataset\n",
    "reader = Reader(rating_scale=(0, clicks_df.click_count.max()))\n",
    "data = Dataset.load_from_df(clicks_df[['user_id', 'click_article_id', 'click_count']], reader)\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_factors': [10, 20, 30],\n",
    "    'n_epochs': [5, 10, 15],\n",
    "    'lr_all': [0.001, 0.002, 0.003],\n",
    "    'reg_all': [0.01, 0.02, 0.03]\n",
    "}\n",
    "\n",
    "# Run a grid search with cross-validation\n",
    "gs = GridSearchCV(SVD, param_grid, measures=['rmse', 'mae'], cv=3)\n",
    "gs.fit(data)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = gs.best_params['rmse']\n",
    "\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "\n",
    "print(f\"Best RMSE: {gs.best_score['rmse']}\")\n",
    "print(f\"Best MAE: {gs.best_score['mae']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_at_k(predictions, k_list=[5, 10]):\n",
    "    '''Return precision and recall at k over all users for multiple values of k'''\n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    user_est_true = defaultdict(list)\n",
    "    for uid, _, true_r, est, _ in predictions:\n",
    "        user_est_true[uid].append((est, true_r))\n",
    "\n",
    "    precisions = {k: dict() for k in k_list}\n",
    "    recalls = {k: dict() for k in k_list}\n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "        # Sort user ratings by estimated value\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        for k in k_list:\n",
    "            # Number of recommended items in top k\n",
    "            n_rec_k = len(user_ratings[:k])\n",
    "\n",
    "            # Number of relevant and recommended items in top k\n",
    "            n_rel_and_rec_k = sum((true_r == est) for (est, true_r) in user_ratings[:k])\n",
    "\n",
    "            # Precision@K: Proportion of recommended items that are relevant\n",
    "            precisions[k][uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 1\n",
    "\n",
    "            # Number of relevant items\n",
    "            n_rel = sum((true_r == est) for (est, true_r) in user_ratings)\n",
    "\n",
    "            # Recall@K: Proportion of relevant items that are recommended\n",
    "            recalls[k][uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 1\n",
    "\n",
    "    return precisions, recalls\n",
    "\n",
    "def collaborativeFilteringRecommendArticle(articles, clicks, user_id, n=5):\n",
    "    # Convert user_id and click_article_id to integer type\n",
    "    clicks['user_id'] = clicks['user_id'].astype(int)\n",
    "    clicks['click_article_id'] = clicks['click_article_id'].astype(int)\n",
    "    articles.index = articles.index.astype(int)\n",
    "    \n",
    "    # Check if user_id is in clicks\n",
    "    if user_id not in clicks['user_id'].values:\n",
    "        return f\"Error: User ID {user_id} not found in clicks data.\"\n",
    "\n",
    "    # Create a new DataFrame that counts the number of times a user clicked on an article\n",
    "    click_counts = clicks.groupby(['user_id', 'click_article_id']).size().reset_index(name='click_count')\n",
    "\n",
    "    # Use a smaller subset of data for the collaborative filtering to avoid memory issues\n",
    "    data_subset = click_counts\n",
    "\n",
    "    # Create a reader and a data object\n",
    "    reader = Reader(rating_scale=(1, data_subset.click_count.max()))  # assuming a click count of at least 1\n",
    "    data = Dataset.load_from_df(data_subset, reader)\n",
    "\n",
    "    # Split the data into train and test sets\n",
    "    trainset, testset = train_test_split(data, test_size=0.2)\n",
    "\n",
    "    # Train a SVD model with the best parameters\n",
    "    algo = SVD(n_factors=best_params['n_factors'],n_epochs=best_params['n_epochs'], lr_all=best_params['lr_all'], reg_all=best_params['reg_all'])\n",
    "    algo.fit(trainset)\n",
    "\n",
    "    # Predict ratings for the testset\n",
    "    predictions_test = algo.test(testset)\n",
    "\n",
    "    # Calculate precision and recall at k\n",
    "    precisions, recalls = precision_recall_at_k(predictions_test, k_list=[5, 10])\n",
    "    for k in [5, 10]:\n",
    "        avg_precision = sum(prec for prec in precisions[k].values()) / len(precisions[k])\n",
    "        avg_recall = sum(rec for rec in recalls[k].values()) / len(recalls[k])\n",
    "        print(f\"Average Precision at {k}: {avg_precision}\")\n",
    "        print(f\"Average Recall at {k}: {avg_recall}\")\n",
    "\n",
    "    # Get the list of articles read by the user\n",
    "    articles_read = clicks[clicks['user_id'] == user_id]['click_article_id'].tolist()\n",
    "\n",
    "    # Get the list of all articles\n",
    "    all_articles = list(articles.index)\n",
    "\n",
    "    # Remove the articles already read by the user\n",
    "    articles_to_predict = [article for article in all_articles if article not in articles_read]\n",
    "\n",
    "    # Get the predicted ratings for the articles not yet read by the user\n",
    "    predictions = {article: algo.predict(user_id, article).est for article in articles_to_predict}\n",
    "\n",
    "    # Get the top n articles\n",
    "    top_n_articles = nlargest(n, predictions, key=predictions.get)\n",
    "\n",
    "    return top_n_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision at 5: 0.3394941377528835\n",
      "Average Recall at 5: 0.9252399546895873\n",
      "Average Precision at 10: 0.35810079658429156\n",
      "Average Recall at 10: 0.9811812621699382\n"
     ]
    }
   ],
   "source": [
    "recommended_articles = collaborativeFilteringRecommendArticle(articles_df, clicks_df, 7723, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended articles for user 7723:\n",
      "article_id      237071\n",
      "category_id        375\n",
      "publisher_id         0\n",
      "words_count        161\n",
      "Name: 237071, dtype: int64\n",
      "article_id      363925\n",
      "category_id        458\n",
      "publisher_id         0\n",
      "words_count        326\n",
      "Name: 363925, dtype: int64\n",
      "article_id      38823\n",
      "category_id        60\n",
      "publisher_id        0\n",
      "words_count       262\n",
      "Name: 38823, dtype: int64\n",
      "article_id      43032\n",
      "category_id        68\n",
      "publisher_id        0\n",
      "words_count       148\n",
      "Name: 43032, dtype: int64\n",
      "article_id      73431\n",
      "category_id       138\n",
      "publisher_id        0\n",
      "words_count       183\n",
      "Name: 73431, dtype: int64\n",
      "article_id      69463\n",
      "category_id       136\n",
      "publisher_id        0\n",
      "words_count       172\n",
      "Name: 69463, dtype: int64\n",
      "article_id      105941\n",
      "category_id        228\n",
      "publisher_id         0\n",
      "words_count        145\n",
      "Name: 105941, dtype: int64\n",
      "article_id      146230\n",
      "category_id        271\n",
      "publisher_id         0\n",
      "words_count        221\n",
      "Name: 146230, dtype: int64\n",
      "article_id      225378\n",
      "category_id        354\n",
      "publisher_id         0\n",
      "words_count        205\n",
      "Name: 225378, dtype: int64\n",
      "article_id      74254\n",
      "category_id       141\n",
      "publisher_id        0\n",
      "words_count       141\n",
      "Name: 74254, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Recommended articles for user 7723:\")\n",
    "for article_id in recommended_articles:\n",
    "    print(articles_df.loc[article_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision at 5: 0.15603441275239588\n",
      "Average Recall at 5: 0.955144725884014\n",
      "Average Precision at 10: 0.1678363519898356\n",
      "Average Recall at 10: 0.9918343063678295\n"
     ]
    }
   ],
   "source": [
    "recommended_articles = collaborativeFilteringRecommendArticle(articles_df, clicks_df, 20 , n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision at 5: 0.1556990868167984\n",
      "Average Recall at 5: 0.9554786949001022\n",
      "Average Precision at 10: 0.16748426520754528\n",
      "Average Recall at 10: 0.9918598440366335\n"
     ]
    }
   ],
   "source": [
    "recommended_articles = collaborativeFilteringRecommendArticle(articles_df, clicks_df, 160974 , n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended articles for user 20:\n",
      "article_id      68851\n",
      "category_id       136\n",
      "publisher_id        0\n",
      "words_count       278\n",
      "Name: 68851, dtype: int64\n",
      "article_id      237071\n",
      "category_id        375\n",
      "publisher_id         0\n",
      "words_count        161\n",
      "Name: 237071, dtype: int64\n",
      "article_id      363925\n",
      "category_id        458\n",
      "publisher_id         0\n",
      "words_count        326\n",
      "Name: 363925, dtype: int64\n",
      "article_id      105941\n",
      "category_id        228\n",
      "publisher_id         0\n",
      "words_count        145\n",
      "Name: 105941, dtype: int64\n",
      "article_id      73431\n",
      "category_id       138\n",
      "publisher_id        0\n",
      "words_count       183\n",
      "Name: 73431, dtype: int64\n",
      "article_id      69463\n",
      "category_id       136\n",
      "publisher_id        0\n",
      "words_count       172\n",
      "Name: 69463, dtype: int64\n",
      "article_id      62197\n",
      "category_id       127\n",
      "publisher_id        0\n",
      "words_count       251\n",
      "Name: 62197, dtype: int64\n",
      "article_id      74254\n",
      "category_id       141\n",
      "publisher_id        0\n",
      "words_count       141\n",
      "Name: 74254, dtype: int64\n",
      "article_id      146230\n",
      "category_id        271\n",
      "publisher_id         0\n",
      "words_count        221\n",
      "Name: 146230, dtype: int64\n",
      "article_id      118347\n",
      "category_id        247\n",
      "publisher_id         0\n",
      "words_count        173\n",
      "Name: 118347, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Recommended articles for user 20:\")\n",
    "for article_id in recommended_articles:\n",
    "    print(articles_df.loc[article_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid based filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybridRecommendArticle(articles, clicks, user_id, n=5):\n",
    "    # Convert user_id and click_article_id to integer type\n",
    "    clicks['user_id'] = clicks['user_id'].astype(int)\n",
    "    clicks['click_article_id'] = clicks['click_article_id'].astype(int)\n",
    "    articles.index = articles.index.astype(int)\n",
    "    \n",
    "    # Check if user_id is in clicks\n",
    "    if user_id not in clicks['user_id'].values:\n",
    "        return f\"Error: User ID {user_id} not found in clicks data.\"\n",
    "\n",
    "    # Create a new DataFrame that counts the number of times a user clicked on an article\n",
    "    click_counts = clicks.groupby(['user_id', 'click_article_id']).size().reset_index(name='click_count')\n",
    "\n",
    "    # Use a smaller subset of data for the collaborative filtering to avoid memory issues\n",
    "    data_subset = click_counts\n",
    "\n",
    "    # Create a reader and a data object\n",
    "    reader = Reader(rating_scale=(1, data_subset.click_count.max()))  # assuming a click count of at least 1\n",
    "    data = Dataset.load_from_df(data_subset, reader)\n",
    "\n",
    "    # Split the data into train and test sets\n",
    "    trainset, testset = train_test_split(data, test_size=0.2)\n",
    "\n",
    "    # Train a SVD model with the best parameters\n",
    "    algo = SVDpp(n_epochs=best_params['n_epochs'], lr_all=best_params['lr_all'], reg_all=best_params['reg_all'])\n",
    "    algo.fit(trainset)\n",
    "\n",
    "    # Predict ratings for the testset\n",
    "    predictions_test = algo.test(testset)\n",
    "\n",
    "    # Calculate precision and recall at k\n",
    "    precisions, recalls = precision_recall_at_k(predictions_test, k_list=[5, 10])\n",
    "    for k in [5, 10]:\n",
    "        avg_precision = sum(prec for prec in precisions[k].values()) / len(precisions[k])\n",
    "        avg_recall = sum(rec for rec in recalls[k].values()) / len(recalls[k])\n",
    "        print(f\"Average Precision at {k}: {avg_precision}\")\n",
    "        print(f\"Average Recall at {k}: {avg_recall}\")\n",
    "\n",
    "    # Get the list of articles read by the user\n",
    "    articles_read = clicks[clicks['user_id'] == user_id]['click_article_id'].tolist()\n",
    "\n",
    "    # Get the list of all articles\n",
    "    all_articles = list(articles.index)\n",
    "\n",
    "    # Remove the articles already read by the user\n",
    "    articles_to_predict = [article for article in all_articles if article not in articles_read]\n",
    "\n",
    "    # Get the predicted ratings for the articles not yet read by the user\n",
    "    predictions = {article: algo.predict(user_id, article).est for article in articles_to_predict}\n",
    "\n",
    "    # Get the top n articles\n",
    "    top_n_articles_collab = nlargest(n, predictions, key=predictions.get)\n",
    "\n",
    "    # Content-based filtering\n",
    "    top_n_articles_content = recommend_articles(articles, clicks, user_id, n)\n",
    "\n",
    "    # Combine the results of collaborative and content-based filtering\n",
    "    top_n_articles = top_n_articles_collab + top_n_articles_content\n",
    "\n",
    "    return top_n_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision at 5: 0.15311180453172374\n",
      "Average Recall at 5: 0.9551493733941426\n",
      "Average Precision at 10: 0.16524886347860998\n",
      "Average Recall at 10: 0.9920455463510742\n",
      "Articles read by user 7723: [214455, 9308, 9649, 129799, 141548, 336220, 353673, 337192, 84763, 107179, 199197, 271400, 84835, 338339, 60252, 303565, 31520, 36685, 36609, 163505, 123434, 141050, 313504, 272660, 72618, 72646, 140445, 277491, 226648, 57740, 128551, 140324, 198659, 166581, 156560, 282964, 225124, 277491, 128707]\n",
      "Number of articles read by user 7723: 39\n",
      "Remaining articles after removing articles read by user 7723: 364009\n",
      "Number of recommended articles that have already been read by user 7723: 0\n"
     ]
    }
   ],
   "source": [
    "recommended_articles = hybridRecommendArticle(articles_df, clicks_df, 7723, n=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projet10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
