{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projet 10: Application de recommandation de contenu\n",
    "### Partie 2: Modèlisation \n",
    "Dans ce notebook on vas procéder à effectuer la modélisation des nos données pour mettre en place une systeme de recommandation du contenu.\n",
    "Nous allons utiliser deux approches principales:\n",
    "- Content-based filtering\n",
    "- Collaborative-based filtering\n",
    "\n",
    "Nous allons nous appuyer sur la libraries [Surpise](https://surpriselib.com/) pour mettre en place nos modèles "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Import des libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from math import floor\n",
    "import numpy as np\n",
    "\n",
    "import surprise\n",
    "from surprise import SVD, Dataset, Reader, KNNBasic, KNNWithMeans\n",
    "from surprise.model_selection import GridSearchCV\n",
    "from surprise.model_selection import train_test_split\n",
    "from heapq import nlargest\n",
    "\n",
    "from surprise import accuracy\n",
    "from surprise import Dataset, Reader\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import SVDpp\n",
    "from collections import defaultdict\n",
    "from heapq import nlargest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Import des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant nous allons procéder aves l'importations des libraries que nous allons utiliser pour la modèlisation et puis on vas importer les fichiers des données que nous allons utiliser comme base pour les modèles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 Définition des chemins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/raw/globocom/\"\n",
    "clicks_path= \"../data/raw/globocom/clicks/\"\n",
    "model_path = \"../model/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 Import fichier avec metadonnées des articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>category_id</th>\n",
       "      <th>publisher_id</th>\n",
       "      <th>words_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>162</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id  category_id  publisher_id  words_count\n",
       "0           0            0             0          168\n",
       "1           1            1             0          189\n",
       "2           2            1             0          250\n",
       "3           3            1             0          230\n",
       "4           4            1             0          162"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_df = pd.read_csv(data_path + 'articles_metadata.csv')\n",
    "articles_df.drop(columns=['created_at_ts'], inplace=True)\n",
    "articles_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3 Import fichiers avec embeddings des articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>240</th>\n",
       "      <th>241</th>\n",
       "      <th>242</th>\n",
       "      <th>243</th>\n",
       "      <th>244</th>\n",
       "      <th>245</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.161183</td>\n",
       "      <td>-0.957233</td>\n",
       "      <td>-0.137944</td>\n",
       "      <td>0.050855</td>\n",
       "      <td>0.830055</td>\n",
       "      <td>0.901365</td>\n",
       "      <td>-0.335148</td>\n",
       "      <td>-0.559561</td>\n",
       "      <td>-0.500603</td>\n",
       "      <td>0.165183</td>\n",
       "      <td>...</td>\n",
       "      <td>0.321248</td>\n",
       "      <td>0.313999</td>\n",
       "      <td>0.636412</td>\n",
       "      <td>0.169179</td>\n",
       "      <td>0.540524</td>\n",
       "      <td>-0.813182</td>\n",
       "      <td>0.286870</td>\n",
       "      <td>-0.231686</td>\n",
       "      <td>0.597416</td>\n",
       "      <td>0.409623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.523216</td>\n",
       "      <td>-0.974058</td>\n",
       "      <td>0.738608</td>\n",
       "      <td>0.155234</td>\n",
       "      <td>0.626294</td>\n",
       "      <td>0.485297</td>\n",
       "      <td>-0.715657</td>\n",
       "      <td>-0.897996</td>\n",
       "      <td>-0.359747</td>\n",
       "      <td>0.398246</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.487843</td>\n",
       "      <td>0.823124</td>\n",
       "      <td>0.412688</td>\n",
       "      <td>-0.338654</td>\n",
       "      <td>0.320787</td>\n",
       "      <td>0.588643</td>\n",
       "      <td>-0.594137</td>\n",
       "      <td>0.182828</td>\n",
       "      <td>0.397090</td>\n",
       "      <td>-0.834364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.619619</td>\n",
       "      <td>-0.972960</td>\n",
       "      <td>-0.207360</td>\n",
       "      <td>-0.128861</td>\n",
       "      <td>0.044748</td>\n",
       "      <td>-0.387535</td>\n",
       "      <td>-0.730477</td>\n",
       "      <td>-0.066126</td>\n",
       "      <td>-0.754899</td>\n",
       "      <td>-0.242004</td>\n",
       "      <td>...</td>\n",
       "      <td>0.454756</td>\n",
       "      <td>0.473184</td>\n",
       "      <td>0.377866</td>\n",
       "      <td>-0.863887</td>\n",
       "      <td>-0.383365</td>\n",
       "      <td>0.137721</td>\n",
       "      <td>-0.810877</td>\n",
       "      <td>-0.447580</td>\n",
       "      <td>0.805932</td>\n",
       "      <td>-0.285284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.740843</td>\n",
       "      <td>-0.975749</td>\n",
       "      <td>0.391698</td>\n",
       "      <td>0.641738</td>\n",
       "      <td>-0.268645</td>\n",
       "      <td>0.191745</td>\n",
       "      <td>-0.825593</td>\n",
       "      <td>-0.710591</td>\n",
       "      <td>-0.040099</td>\n",
       "      <td>-0.110514</td>\n",
       "      <td>...</td>\n",
       "      <td>0.271535</td>\n",
       "      <td>0.036040</td>\n",
       "      <td>0.480029</td>\n",
       "      <td>-0.763173</td>\n",
       "      <td>0.022627</td>\n",
       "      <td>0.565165</td>\n",
       "      <td>-0.910286</td>\n",
       "      <td>-0.537838</td>\n",
       "      <td>0.243541</td>\n",
       "      <td>-0.885329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.279052</td>\n",
       "      <td>-0.972315</td>\n",
       "      <td>0.685374</td>\n",
       "      <td>0.113056</td>\n",
       "      <td>0.238315</td>\n",
       "      <td>0.271913</td>\n",
       "      <td>-0.568816</td>\n",
       "      <td>0.341194</td>\n",
       "      <td>-0.600554</td>\n",
       "      <td>-0.125644</td>\n",
       "      <td>...</td>\n",
       "      <td>0.238286</td>\n",
       "      <td>0.809268</td>\n",
       "      <td>0.427521</td>\n",
       "      <td>-0.615932</td>\n",
       "      <td>-0.503697</td>\n",
       "      <td>0.614450</td>\n",
       "      <td>-0.917760</td>\n",
       "      <td>-0.424061</td>\n",
       "      <td>0.185484</td>\n",
       "      <td>-0.580292</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 250 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0 -0.161183 -0.957233 -0.137944  0.050855  0.830055  0.901365 -0.335148   \n",
       "1 -0.523216 -0.974058  0.738608  0.155234  0.626294  0.485297 -0.715657   \n",
       "2 -0.619619 -0.972960 -0.207360 -0.128861  0.044748 -0.387535 -0.730477   \n",
       "3 -0.740843 -0.975749  0.391698  0.641738 -0.268645  0.191745 -0.825593   \n",
       "4 -0.279052 -0.972315  0.685374  0.113056  0.238315  0.271913 -0.568816   \n",
       "\n",
       "        7         8         9    ...       240       241       242       243  \\\n",
       "0 -0.559561 -0.500603  0.165183  ...  0.321248  0.313999  0.636412  0.169179   \n",
       "1 -0.897996 -0.359747  0.398246  ... -0.487843  0.823124  0.412688 -0.338654   \n",
       "2 -0.066126 -0.754899 -0.242004  ...  0.454756  0.473184  0.377866 -0.863887   \n",
       "3 -0.710591 -0.040099 -0.110514  ...  0.271535  0.036040  0.480029 -0.763173   \n",
       "4  0.341194 -0.600554 -0.125644  ...  0.238286  0.809268  0.427521 -0.615932   \n",
       "\n",
       "        244       245       246       247       248       249  \n",
       "0  0.540524 -0.813182  0.286870 -0.231686  0.597416  0.409623  \n",
       "1  0.320787  0.588643 -0.594137  0.182828  0.397090 -0.834364  \n",
       "2 -0.383365  0.137721 -0.810877 -0.447580  0.805932 -0.285284  \n",
       "3  0.022627  0.565165 -0.910286 -0.537838  0.243541 -0.885329  \n",
       "4 -0.503697  0.614450 -0.917760 -0.424061  0.185484 -0.580292  \n",
       "\n",
       "[5 rows x 250 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ouvrir le fichiers pickle et afficher les 5 premiers lignes\n",
    "with open(data_path + 'articles_embeddings.pickle', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "embeddings_df = pd.DataFrame(data)\n",
    "embeddings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=10)\n",
    "pca.fit(embeddings_df)\n",
    "embeddings_df_pca = pca.transform(embeddings_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emb_0</th>\n",
       "      <th>emb_1</th>\n",
       "      <th>emb_2</th>\n",
       "      <th>emb_3</th>\n",
       "      <th>emb_4</th>\n",
       "      <th>emb_5</th>\n",
       "      <th>emb_6</th>\n",
       "      <th>emb_7</th>\n",
       "      <th>emb_8</th>\n",
       "      <th>emb_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2.176782</td>\n",
       "      <td>1.316913</td>\n",
       "      <td>-1.029048</td>\n",
       "      <td>0.901908</td>\n",
       "      <td>-1.809542</td>\n",
       "      <td>-2.064713</td>\n",
       "      <td>-1.221915</td>\n",
       "      <td>-0.024441</td>\n",
       "      <td>0.927261</td>\n",
       "      <td>-0.669806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.735174</td>\n",
       "      <td>-0.489893</td>\n",
       "      <td>3.268562</td>\n",
       "      <td>0.087855</td>\n",
       "      <td>1.473059</td>\n",
       "      <td>-0.932711</td>\n",
       "      <td>1.841631</td>\n",
       "      <td>-0.881798</td>\n",
       "      <td>-0.207201</td>\n",
       "      <td>0.816809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.912688</td>\n",
       "      <td>2.089339</td>\n",
       "      <td>1.865869</td>\n",
       "      <td>-1.202519</td>\n",
       "      <td>2.530600</td>\n",
       "      <td>-0.521970</td>\n",
       "      <td>0.224352</td>\n",
       "      <td>1.479935</td>\n",
       "      <td>-0.191900</td>\n",
       "      <td>1.356800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.096567</td>\n",
       "      <td>-0.212955</td>\n",
       "      <td>4.183517</td>\n",
       "      <td>-0.649575</td>\n",
       "      <td>-0.130867</td>\n",
       "      <td>1.126555</td>\n",
       "      <td>1.063997</td>\n",
       "      <td>-0.662875</td>\n",
       "      <td>0.348143</td>\n",
       "      <td>1.463898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.193785</td>\n",
       "      <td>0.263949</td>\n",
       "      <td>1.896583</td>\n",
       "      <td>-1.834345</td>\n",
       "      <td>1.270377</td>\n",
       "      <td>-1.723296</td>\n",
       "      <td>0.329006</td>\n",
       "      <td>0.283794</td>\n",
       "      <td>-0.659809</td>\n",
       "      <td>1.223740</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      emb_0     emb_1     emb_2     emb_3     emb_4     emb_5     emb_6  \\\n",
       "0 -2.176782  1.316913 -1.029048  0.901908 -1.809542 -2.064713 -1.221915   \n",
       "1 -1.735174 -0.489893  3.268562  0.087855  1.473059 -0.932711  1.841631   \n",
       "2 -0.912688  2.089339  1.865869 -1.202519  2.530600 -0.521970  0.224352   \n",
       "3  1.096567 -0.212955  4.183517 -0.649575 -0.130867  1.126555  1.063997   \n",
       "4  0.193785  0.263949  1.896583 -1.834345  1.270377 -1.723296  0.329006   \n",
       "\n",
       "      emb_7     emb_8     emb_9  \n",
       "0 -0.024441  0.927261 -0.669806  \n",
       "1 -0.881798 -0.207201  0.816809  \n",
       "2  1.479935 -0.191900  1.356800  \n",
       "3 -0.662875  0.348143  1.463898  \n",
       "4  0.283794 -0.659809  1.223740  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_df_pca = pd.DataFrame(embeddings_df_pca, columns=[\"emb_\" + str(i) for i in range(embeddings_df_pca.shape[1])])\n",
    "embeddings_df_pca.to_pickle('../data/processed/embeddings_df_pca.pkl')\n",
    "embeddings_df_pca.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.4 Import fichier avec les interactions des utilisateurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_files_clicks(path):\n",
    "    clicks_df = pd.DataFrame()\n",
    "    for file in os.listdir(path):\n",
    "        df = pd.read_csv(path + file)\n",
    "        clicks_df = pd.concat([clicks_df, df], axis=0)\n",
    "\n",
    "    return clicks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "clicks_df = get_all_files_clicks(clicks_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>session_id</th>\n",
       "      <th>session_start</th>\n",
       "      <th>session_size</th>\n",
       "      <th>click_article_id</th>\n",
       "      <th>click_timestamp</th>\n",
       "      <th>click_environment</th>\n",
       "      <th>click_deviceGroup</th>\n",
       "      <th>click_os</th>\n",
       "      <th>click_country</th>\n",
       "      <th>click_region</th>\n",
       "      <th>click_referrer_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>93863</td>\n",
       "      <td>1507865792177843</td>\n",
       "      <td>2017-10-13 03:36:32</td>\n",
       "      <td>2</td>\n",
       "      <td>96210</td>\n",
       "      <td>2017-10-13 03:37:12.925</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>93863</td>\n",
       "      <td>1507865792177843</td>\n",
       "      <td>2017-10-13 03:36:32</td>\n",
       "      <td>2</td>\n",
       "      <td>158094</td>\n",
       "      <td>2017-10-13 03:37:42.925</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>294036</td>\n",
       "      <td>1507865795185844</td>\n",
       "      <td>2017-10-13 03:36:35</td>\n",
       "      <td>2</td>\n",
       "      <td>20691</td>\n",
       "      <td>2017-10-13 03:36:59.095</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>294036</td>\n",
       "      <td>1507865795185844</td>\n",
       "      <td>2017-10-13 03:36:35</td>\n",
       "      <td>2</td>\n",
       "      <td>96210</td>\n",
       "      <td>2017-10-13 03:37:29.095</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>77136</td>\n",
       "      <td>1507865796257845</td>\n",
       "      <td>2017-10-13 03:36:36</td>\n",
       "      <td>2</td>\n",
       "      <td>336245</td>\n",
       "      <td>2017-10-13 03:42:13.178</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_id        session_id       session_start session_size click_article_id  \\\n",
       "0   93863  1507865792177843 2017-10-13 03:36:32            2            96210   \n",
       "1   93863  1507865792177843 2017-10-13 03:36:32            2           158094   \n",
       "2  294036  1507865795185844 2017-10-13 03:36:35            2            20691   \n",
       "3  294036  1507865795185844 2017-10-13 03:36:35            2            96210   \n",
       "4   77136  1507865796257845 2017-10-13 03:36:36            2           336245   \n",
       "\n",
       "          click_timestamp click_environment click_deviceGroup click_os  \\\n",
       "0 2017-10-13 03:37:12.925                 4                 3        2   \n",
       "1 2017-10-13 03:37:42.925                 4                 3        2   \n",
       "2 2017-10-13 03:36:59.095                 4                 3       20   \n",
       "3 2017-10-13 03:37:29.095                 4                 3       20   \n",
       "4 2017-10-13 03:42:13.178                 4                 3        2   \n",
       "\n",
       "  click_country click_region click_referrer_type  \n",
       "0             1           21                   2  \n",
       "1             1           21                   2  \n",
       "2             1            9                   2  \n",
       "3             1            9                   2  \n",
       "4             1           25                   2  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clicks_df['click_timestamp'] = pd.to_datetime(clicks_df['click_timestamp'], unit='ms')\n",
    "clicks_df['session_start'] = pd.to_datetime(clicks_df['session_start'], unit='ms')\n",
    "clicks_df.to_pickle('../data/processed/clicks_df.pkl')\n",
    "clicks_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Content-based Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le **Content-based Filtering** est une méthode de recommandation qui utilise des informations détaillées sur les éléments pour recommander d'autres éléments similaires. Par exemple, dans un système de recommandation de films, le filtrage basé sur le contenu pourrait utiliser des informations telles que le genre du film, le réalisateur, les acteurs, etc.\n",
    "\n",
    "**Principe**\n",
    "L'idée est que si un utilisateur a aimé un certain élément dans le passé, il est probable qu'il aimera à nouveau des éléments similaires à l'avenir. Par conséquent, le système recommande des éléments qui sont similaires aux éléments que l'utilisateur a aimés précédemment.\n",
    "\n",
    "**Calcul de la similarité**\n",
    "La similarité entre les éléments est généralement calculée en utilisant des techniques telles que la similarité cosinus ou la distance euclidienne. Les éléments qui sont les plus similaires à ceux que l'utilisateur a aimés sont recommandés.\n",
    "\n",
    "**Note importante**\n",
    "Il est important de noter que le filtrage basé sur le contenu ne tient pas compte des opinions d'autres utilisateurs. Il se concentre uniquement sur les préférences de l'utilisateur actuel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Étapes du Code \n",
    "\n",
    "1. **Préparation des Données** :\n",
    "    - Les colonnes `user_id` et `click_article_id` sont converties en type entier.\n",
    "    - Les articles lus par l'utilisateur cible sont identifiés.\n",
    "    - Si l'utilisateur n'a lu aucun article, les articles les plus populaires sont recommandés.\n",
    "\n",
    "2. **Filtrage Basé sur le Contenu** :\n",
    "    - Les embeddings des articles lus par l'utilisateur sont obtenus.\n",
    "    - Les articles lus par l'utilisateur sont retirés de la liste des articles.\n",
    "    - La similarité cosinus entre les articles lus par l'utilisateur et les autres articles est calculée.\n",
    "    - Les articles les plus similaires aux articles lus par l'utilisateur sont recommandés.\n",
    "    - La similarité des articles recommandés est mise à zéro pour éviter de les recommander à nouveau.\n",
    "\n",
    "3. **Vérification des Résultats** :\n",
    "    - Le nombre d'articles recommandés qui ont déjà été lus par l'utilisateur est calculé et affiché.\n",
    "    - Les recommandations sont retournées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_articles(articles, clicks, user_id, n=5):\n",
    "    # Convert user_id and click_article_id to integer type\n",
    "    clicks['user_id'] = clicks['user_id'].astype(int)\n",
    "    clicks['click_article_id'] = clicks['click_article_id'].astype(int)\n",
    "    articles.index = articles.index.astype(int)\n",
    "    \n",
    "    # Get the articles read by the user\n",
    "    articles_read = clicks[clicks['user_id'] == int(user_id)]['click_article_id'].tolist()\n",
    "    print(f\"Articles read by user {user_id}: {articles_read}\")\n",
    "\n",
    "    # If the user hasn't read any articles, recommend the most popular ones\n",
    "    if len(articles_read) == 0:\n",
    "        most_popular_articles = clicks['click_article_id'].value_counts().index.tolist()\n",
    "        print(f\"User {user_id} has not read any articles. Recommending most popular articles: {most_popular_articles[:n]}\")\n",
    "        return most_popular_articles[:n]\n",
    "\n",
    "    # Get the embeddings of the articles read by the user\n",
    "    articles_read_embedding = articles.loc[articles_read]\n",
    "    print(f\"Number of articles read by user {user_id}: {len(articles_read)}\")\n",
    "\n",
    "    # Remove the articles read by the user from the list of articles\n",
    "    articles = articles.drop(articles_read)\n",
    "    print(f\"Remaining articles after removing articles read by user {user_id}: {len(articles)}\")\n",
    "\n",
    "    # Calculate the cosine similarity between the articles read by the user and the other articles\n",
    "    matrix = cosine_similarity(articles_read_embedding, articles)\n",
    "\n",
    "    recommendations = []\n",
    "\n",
    "    # Recommend the articles most similar to the articles read by the user\n",
    "    for i in range(n):\n",
    "        coord_x = floor(np.argmax(matrix)/matrix.shape[1])\n",
    "        coord_y = np.argmax(matrix)%matrix.shape[1]\n",
    "\n",
    "        recommendations.append(int(articles.index[coord_y]))\n",
    "\n",
    "        # Set the similarity of the recommended article to 0\n",
    "        matrix[coord_x][coord_y] = 0\n",
    "\n",
    "    # Print the number of recommended articles that have already been read by the user\n",
    "    already_read = len(set(recommendations) & set(articles_read))\n",
    "    print(f\"Number of recommended articles that have already been read by user {user_id}: {already_read}\")\n",
    "\n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = 7723"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Articles read by user 7723: [214455, 9308, 9649, 129799, 141548, 336220, 353673, 337192, 84763, 107179, 199197, 271400, 84835, 338339, 60252, 303565, 31520, 36685, 36609, 163505, 123434, 141050, 313504, 272660, 72618, 72646, 140445, 277491, 226648, 57740, 128551, 140324, 198659, 166581, 156560, 282964, 225124, 277491, 128707]\n",
      "Number of articles read by user 7723: 39\n",
      "Remaining articles after removing articles read by user 7723: 364009\n",
      "Number of recommended articles that have already been read by user 7723: 0\n",
      "recommended articles: [141742, 128643, 127781, 139329, 85765, 139598, 89224, 139886, 34677, 128120]\n"
     ]
    }
   ],
   "source": [
    "recommend = recommend_articles(embeddings_df_pca, clicks_df, user_id, 10)\n",
    "print(f\"recommended articles: {recommend}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Articles read by user 7723: [214455, 9308, 9649, 129799, 141548, 336220, 353673, 337192, 84763, 107179, 199197, 271400, 84835, 338339, 60252, 303565, 31520, 36685, 36609, 163505, 123434, 141050, 313504, 272660, 72618, 72646, 140445, 277491, 226648, 57740, 128551, 140324, 198659, 166581, 156560, 282964, 225124, 277491, 128707]\n",
      "Number of articles read by user 7723: 39\n",
      "Remaining articles after removing articles read by user 7723: 364009\n",
      "Number of recommended articles that have already been read by user 7723: 0\n",
      "recommended articles: [124988, 139886, 129016, 125516, 140603, 125546, 141291, 140328, 140815, 127495]\n"
     ]
    }
   ],
   "source": [
    "recommend = recommend_articles(embeddings_df, clicks_df, user_id, 10)\n",
    "print(f\"recommended articles: {recommend}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Collaborative-based Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le **Collaborative-based Filtering** est une méthode de recommandation qui se base sur les comportements passés des utilisateurs pour faire des prédictions sur ce qu'un utilisateur pourrait aimer.\n",
    "\n",
    "**Principe**\n",
    "L'idée principale est que si deux utilisateurs ont eu des comportements similaires par le passé (par exemple, ils ont aimé les mêmes films ou acheté les mêmes produits), alors ils sont susceptibles d'avoir des intérêts similaires à l'avenir.\n",
    "\n",
    "**Types de Filtrage Collaboratif**\n",
    "Il existe deux types principaux de filtrage collaboratif :\n",
    "\n",
    "1. **Filtrage Collaboratif Basé sur les Utilisateurs** : Cette méthode trouve des utilisateurs similaires à l'utilisateur cible et recommande des éléments que ces utilisateurs similaires ont aimés.\n",
    "\n",
    "2. **Filtrage Collaboratif Basé sur les Éléments** : Cette méthode trouve des éléments similaires à ceux que l'utilisateur cible a aimés et recommande ces éléments similaires.\n",
    "\n",
    "3. **Filtrage Collaboratif Basé sur un Modèle** : Cette méthode utilise des techniques de modélisation, comme la factorisation de matrices ou le clustering, pour prédire l'intérêt d'un utilisateur pour un élément. Elle se base sur les comportements passés de tous les utilisateurs, ainsi que sur les évaluations que l'utilisateur cible a données à d'autres éléments.\n",
    "\n",
    "**Calcul de la similarité**\n",
    "La similarité entre les utilisateurs ou les éléments est généralement calculée en utilisant des techniques telles que la corrélation de Pearson ou la similarité cosinus.\n",
    "\n",
    "**Note importante**\n",
    "Contrairement au filtrage basé sur le contenu, le filtrage collaboratif ne nécessite pas d'informations détaillées sur les éléments. Il se base uniquement sur les interactions passées entre les utilisateurs et les éléments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Collaborative Filtering User Based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Étapes du Code \n",
    "\n",
    "1. **Préparation des Données** :\n",
    "    - Les colonnes `user_id` et `click_article_id` sont converties en type entier.\n",
    "    - Les articles lus par l'utilisateur cible sont identifiés.\n",
    "\n",
    "2. **Filtrage Collaboratif** :\n",
    "    - Les utilisateurs qui ont lu les mêmes articles que l'utilisateur cible sont identifiés.\n",
    "    - Les articles lus par ces utilisateurs similaires sont identifiés.\n",
    "    - Les articles déjà lus par l'utilisateur cible sont retirés de ces recommandations.\n",
    "\n",
    "3. **Vérification des Résultats** :\n",
    "    - Si le nombre de recommandations est inférieur à n, les articles les plus populaires sont ajoutés aux recommandations jusqu'à atteindre n.\n",
    "    - Les n premières recommandations sont retournées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collaborativeFilteringRecommendArticle(clicks, user_id, n=5):\n",
    "    # Convert user_id and click_article_id to integer type\n",
    "    clicks['user_id'] = clicks['user_id'].astype(int)\n",
    "    clicks['click_article_id'] = clicks['click_article_id'].astype(int)\n",
    "    \n",
    "    # Get the articles read by the target user\n",
    "    articles_read = clicks[clicks['user_id'] == user_id]['click_article_id'].values\n",
    "    \n",
    "    # Get the users who have read the same articles as the target user\n",
    "    similar_users = clicks[clicks['click_article_id'].isin(articles_read)]['user_id'].unique()\n",
    "    \n",
    "    # Get the articles read by the similar users\n",
    "    similar_users_articles = clicks[clicks['user_id'].isin(similar_users)]['click_article_id'].unique()\n",
    "    \n",
    "    # Remove the articles already read by the target user\n",
    "    recommendations = [article for article in similar_users_articles if article not in articles_read]\n",
    "    \n",
    "    # If there are not enough recommendations, add the most popular articles\n",
    "    if len(recommendations) < n:\n",
    "        most_popular_articles = clicks['click_article_id'].value_counts().index.tolist()\n",
    "        for article in most_popular_articles:\n",
    "            if article not in recommendations and article not in articles_read:\n",
    "                recommendations.append(article)\n",
    "            if len(recommendations) == n:\n",
    "                break\n",
    "    \n",
    "    return recommendations[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[96210, 158094, 336245, 159197, 337441]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collaborativeFilteringRecommendArticle(clicks_df,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Collaborative Filtering Model Based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`GridSearchCV` est une méthode de recherche exhaustive qui parcourt toutes les combinaisons possibles de paramètres pour trouver celle qui produit le meilleur score de validation croisée.\n",
    "\n",
    "`GridSearchCV` fonctionne en entraînant et en évaluant un modèle pour chaque combinaison de paramètres. Il utilise la validation croisée pour évaluer la performance du modèle, ce qui signifie qu'il divise les données en un ensemble d'entraînement et un ensemble de test, entraîne le modèle sur l'ensemble d'entraînement, puis évalue la performance sur l'ensemble de test.\n",
    "\n",
    "Une fois que `GridSearchCV` a terminé la recherche, nous pouvons obtenir les meilleurs paramètres en utilisant l'attribut `best_params_`. Nous pouvons ensuite utiliser ces paramètres pour entraîner notre modèle final."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 GridSearchCV\n",
    "\n",
    "1. **Création de la Colonne 'click_count'** :\n",
    "    Nous ajoutons une colonne click_count au DataFrame clicks_df qui compte le nombre de clics pour chaque article par utilisateur.\n",
    "\n",
    "2. **Chargement d'une Fraction des Données** :\n",
    "    Nous chargeons une fraction (10%) des données dans un dataset Surprise. Cela est fait en utilisant les colonnes user_id, click_article_id et click_count.\n",
    "\n",
    "3. **Définition des Modèles et Paramètres** :\n",
    "    Nous définissons différents modèles de prédiction ainsi que leurs grilles de paramètres respectifs pour la recherche de la meilleure configuration.\n",
    "\n",
    "4. **Recherche des Meilleurs Paramètres** :\n",
    "    Pour chaque modèle, nous utilisons GridSearchCV pour trouver les meilleurs paramètres en termes de RMSE et MAE à travers une validation croisée à 3 plis.\n",
    "\n",
    "5. **Sélection du Meilleur Modèle** :\n",
    "    Nous comparons les scores RMSE obtenus par chaque modèle et conservons le modèle avec le meilleur score ainsi que ses paramètres.\n",
    "\n",
    "6. **Affichage des Résultats** :\n",
    "    Nous affichons les meilleurs paramètres et les scores (RMSE et MAE) pour chaque modèle, ainsi que le meilleur modèle global.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for SVD: {'n_epochs': 10, 'lr_all': 0.005, 'reg_all': 0.4}\n",
      "Best RMSE for SVD: 0.23938850511580548\n",
      "Best MAE for SVD: 0.056271097415754666\n",
      "Best parameters for SVDpp: {'n_epochs': 10, 'lr_all': 0.005, 'reg_all': 0.4}\n",
      "Best RMSE for SVDpp: 0.23561784625022433\n",
      "Best MAE for SVDpp: 0.05295362785914575\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Best parameters for KNNWithMeans: {'k': 20, 'sim_options': {'name': 'msd', 'user_based': False}}\n",
      "Best RMSE for KNNWithMeans: 0.24423153346860757\n",
      "Best MAE for KNNWithMeans: 0.06371725768887639\n",
      "Best parameters for CoClustering: {'n_cltr_u': 3, 'n_cltr_i': 5}\n",
      "Best RMSE for CoClustering: 0.7482393592705261\n",
      "Best MAE for CoClustering: 0.13818112155832993\n",
      "Best parameters for SlopeOne: {}\n",
      "Best RMSE for SlopeOne: 0.2920311538513885\n",
      "Best MAE for SlopeOne: 0.08266460143474147\n",
      "Best parameters for NormalPredictor: {}\n",
      "Best RMSE for NormalPredictor: 0.44772119033705743\n",
      "Best MAE for NormalPredictor: 0.2765924737738579\n",
      "\n",
      "Best model: SVDpp\n",
      "Best parameters: {'n_epochs': 10, 'lr_all': 0.005, 'reg_all': 0.4}\n",
      "Best RMSE: 0.23561784625022433\n"
     ]
    }
   ],
   "source": [
    "from surprise import SVD, SVDpp, KNNWithMeans, CoClustering, SlopeOne, NormalPredictor\n",
    "from surprise.model_selection import GridSearchCV\n",
    "\n",
    "# Create a 'click_count' column\n",
    "clicks_df['click_count'] = clicks_df.groupby(['user_id', 'click_article_id'])['click_timestamp'].transform('count')\n",
    "\n",
    "# Load a fraction of the data into a Surprise dataset\n",
    "reader = Reader(rating_scale=(0, clicks_df.click_count.max()))\n",
    "data = Dataset.load_from_df(clicks_df[['user_id', 'click_article_id', 'click_count']].sample(frac=0.1, random_state=42), reader)\n",
    "\n",
    "\n",
    "models = {\n",
    "    SVD: {'n_epochs': [5, 10], 'lr_all': [0.002, 0.005], 'reg_all': [0.4, 0.6]},\n",
    "    SVDpp: {'n_epochs': [5, 10], 'lr_all': [0.002, 0.005], 'reg_all': [0.4, 0.6]},\n",
    "    KNNWithMeans: {'k': [20], 'sim_options': {'name': ['msd', 'cosine'], 'user_based': [False]}},\n",
    "    CoClustering: {'n_cltr_u': [3, 5], 'n_cltr_i': [3, 5]},\n",
    "    SlopeOne: {},\n",
    "    NormalPredictor: {}\n",
    "}\n",
    "\n",
    "best_score = float('inf')\n",
    "best_model = None\n",
    "best_params = None\n",
    "\n",
    "for model, param_grid in models.items():\n",
    "    gs = GridSearchCV(model, param_grid, measures=['rmse', 'mae'], cv=3)\n",
    "    gs.fit(data)\n",
    "    params = gs.best_params['rmse']\n",
    "    score = gs.best_score['rmse']\n",
    "    print(f\"Best parameters for {model.__name__}: {params}\")\n",
    "    print(f\"Best RMSE for {model.__name__}: {score}\")\n",
    "    print(f\"Best MAE for {model.__name__}: {gs.best_score['mae']}\")\n",
    "    if score < best_score:\n",
    "        best_score = score\n",
    "        best_model = model.__name__\n",
    "        best_params = params\n",
    "\n",
    "print(f\"\\nBest model: {best_model}\")\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "print(f\"Best RMSE: {best_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 SVD\n",
    "La décomposition en valeurs singulières (SVD, de l'anglais \"Singular Value Decomposition\") est une technique utilisée en réduction de dimension et en apprentissage automatique, notamment pour les systèmes de recommandation. Elle décompose une matrice en trois autres matrices, permettant ainsi de capturer les relations latentes entre les utilisateurs et les articles. En réduisant les dimensions, SVD aide à prédire les évaluations manquantes et à faire des recommandations plus précises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Création d'une Colonne 'click_count'** :\n",
    "Tout d'abord, nous ajoutons une nouvelle colonne appelée click_count au DataFrame clicks_df. Cette colonne compte le nombre de clics que chaque utilisateur a effectués sur chaque article. Pour cela, nous utilisons la méthode groupby pour regrouper les données par user_id et click_article_id, puis nous appliquons la fonction transform pour calculer le nombre de clics.\n",
    "\n",
    "2. **Chargement des Données** :\n",
    "Ensuite, nous chargeons les données dans un dataset compatible avec la bibliothèque Surprise. Nous définissons une plage de notation allant de 0 au nombre maximum de clics observé dans click_count. Nous sélectionnons ensuite les colonnes pertinentes (user_id, click_article_id, click_count) pour créer ce dataset.\n",
    "\n",
    "3. **Définition de la Grille de Paramètres** :\n",
    "Nous définissons une grille de paramètres pour le modèle SVD. Cette grille spécifie différentes valeurs possibles pour plusieurs hyperparamètres :\n",
    "- n_factors : le nombre de facteurs latents à utiliser.\n",
    "- n_epochs : le nombre d'époques d'entraînement.\n",
    "- lr_all : le taux d'apprentissage.\n",
    "- reg_all : les paramètres de régularisation.\n",
    "\n",
    "4. **Recherche en Grille avec Validation Croisée** :\n",
    "Nous utilisons la méthode GridSearchCV de Surprise pour effectuer une recherche en grille des meilleurs paramètres. Cette méthode évalue les différentes combinaisons d'hyperparamètres en utilisant une validation croisée à 3 plis et en mesurant les performances avec deux métriques : RMSE (Root Mean Square Error) et MAE (Mean Absolute Error).\n",
    "\n",
    "5. **Obtention et Affichage des Meilleurs Paramètres** :\n",
    "Une fois la recherche en grille terminée, nous récupérons les meilleurs paramètres trouvés en termes de RMSE. Nous affichons ensuite ces paramètres ainsi que les scores de RMSE et MAE obtenus avec ces paramètres optimaux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'n_factors': 30, 'n_epochs': 10, 'lr_all': 0.003, 'reg_all': 0.01}\n",
      "Best RMSE: 0.18807730866826675\n",
      "Best MAE: 0.04740245811879407\n"
     ]
    }
   ],
   "source": [
    "# Create a 'click_count' column\n",
    "clicks_df['click_count'] = clicks_df.groupby(['user_id', 'click_article_id'])['click_timestamp'].transform('count')\n",
    "\n",
    "# Load a fraction of the data into a Surprise dataset\n",
    "reader = Reader(rating_scale=(0, clicks_df.click_count.max()))\n",
    "data = Dataset.load_from_df(clicks_df[['user_id', 'click_article_id', 'click_count']], reader)\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_factors': [10, 20, 30],\n",
    "    'n_epochs': [5, 10],\n",
    "    'lr_all': [0.001, 0.002, 0.003],\n",
    "    'reg_all': [0.01, 0.02]\n",
    "}\n",
    "\n",
    "# Run a grid search with cross-validation\n",
    "gs = GridSearchCV(SVD, param_grid, measures=['rmse', 'mae'], cv=3)\n",
    "gs.fit(data)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = gs.best_params['rmse']\n",
    "\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "\n",
    "print(f\"Best RMSE: {gs.best_score['rmse']}\")\n",
    "print(f\"Best MAE: {gs.best_score['mae']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<surprise.prediction_algorithms.matrix_factorization.SVD at 0x3f63abe90>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model with the best parameters\n",
    "model = SVD(**best_params)\n",
    "trainset = data.build_full_trainset()\n",
    "model.fit(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "filename = '../model/finalized_model.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.3.1 Définition de Précision@k et Rappel@k\n",
    "\n",
    "##### Précision@k\n",
    "\n",
    "La **précision@k** est une mesure qui évalue la proportion des éléments recommandés parmi les k premiers qui sont pertinents. En d'autres termes, elle indique la qualité des recommandations en termes de pertinence des k premières suggestions. Dans ce code, la précision@k est calculée comme suit :\n",
    "\n",
    "1. On trie les prédictions par valeur estimée décroissante pour chaque utilisateur.\n",
    "2. On prend les k premières prédictions.\n",
    "3. On calcule le nombre d'éléments recommandés et pertinents parmi ces k premiers (`n_rel_and_rec_k`).\n",
    "4. La précision@k est la proportion de ces éléments pertinents par rapport au nombre total d'éléments recommandés dans les k premiers (`n_rec_k`).\n",
    "\n",
    "La formule est donc :\n",
    "\n",
    "Précision@k = (Nombre d'éléments pertinents parmi les k premiers) / (Nombre total d'éléments recommandés dans les k premiers)\n",
    "\n",
    "##### Rappel@k\n",
    "\n",
    "Le **rappel@k** est une mesure qui évalue la proportion des éléments pertinents qui sont recommandés parmi les k premiers. En d'autres termes, elle indique la couverture des recommandations en termes de pertinence. Dans ce code, le rappel@k est calculé comme suit :\n",
    "\n",
    "1. On trie les prédictions par valeur estimée décroissante pour chaque utilisateur.\n",
    "2. On prend les k premières prédictions.\n",
    "3. On calcule le nombre d'éléments recommandés et pertinents parmi ces k premiers (`n_rel_and_rec_k`).\n",
    "4. On calcule le nombre total d'éléments pertinents pour l'utilisateur (`n_rel`).\n",
    "5. Le rappel@k est la proportion des éléments pertinents recommandés parmi les k premiers par rapport au nombre total d'éléments pertinents.\n",
    "\n",
    "La formule est donc :\n",
    "\n",
    "Rappel@k = (Nombre d'éléments pertinents parmi les k premiers) / (Nombre total d'éléments pertinents)\n",
    "\n",
    "#### En synthèse\n",
    "\n",
    "- **Précision@k** mesure la qualité des recommandations parmi les k premiers éléments recommandés.\n",
    "- **Rappel@k** mesure la couverture des éléments pertinents parmi les k premiers éléments recommandés.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_at_k(predictions, k_list=[5, 10]):\n",
    "    '''Return precision and recall at k over all users for multiple values of k'''\n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    user_est_true = defaultdict(list)\n",
    "    for uid, _, true_r, est, _ in predictions:\n",
    "        user_est_true[uid].append((est, true_r))\n",
    "\n",
    "    precisions = {k: dict() for k in k_list}\n",
    "    recalls = {k: dict() for k in k_list}\n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "        # Sort user ratings by estimated value\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        for k in k_list:\n",
    "            # Number of recommended items in top k\n",
    "            n_rec_k = len(user_ratings[:k])\n",
    "\n",
    "            # Number of relevant and recommended items in top k\n",
    "            n_rel_and_rec_k = sum((true_r == est) for (est, true_r) in user_ratings[:k])\n",
    "\n",
    "            # Precision@K: Proportion of recommended items that are relevant\n",
    "            precisions[k][uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 1\n",
    "\n",
    "            # Number of relevant items\n",
    "            n_rel = sum((true_r == est) for (est, true_r) in user_ratings)\n",
    "\n",
    "            # Recall@K: Proportion of relevant items that are recommended\n",
    "            recalls[k][uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 1\n",
    "\n",
    "    return precisions, recalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collaborativeFilteringRecommendArticle(articles, clicks, user_id, n=5):\n",
    "    # Convert user_id and click_article_id to integer type\n",
    "    clicks['user_id'] = clicks['user_id'].astype(int)\n",
    "    clicks['click_article_id'] = clicks['click_article_id'].astype(int)\n",
    "    articles.index = articles.index.astype(int)\n",
    "    \n",
    "    # Check if user_id is in clicks\n",
    "    if user_id not in clicks['user_id'].values:\n",
    "        return f\"Error: User ID {user_id} not found in clicks data.\"\n",
    "\n",
    "    # Create a new DataFrame that counts the number of times a user clicked on an article\n",
    "    click_counts = clicks.groupby(['user_id', 'click_article_id']).size().reset_index(name='click_count')\n",
    "\n",
    "    # Use a smaller subset of data for the collaborative filtering to avoid memory issues\n",
    "    data_subset = click_counts\n",
    "\n",
    "    # Create a reader and a data object\n",
    "    reader = Reader(rating_scale=(1, data_subset.click_count.max()))  # assuming a click count of at least 1\n",
    "    data = Dataset.load_from_df(data_subset, reader)\n",
    "\n",
    "    # Split the data into train and test sets\n",
    "    trainset, testset = train_test_split(data, test_size=0.2)\n",
    "\n",
    "    # Train a SVD model with the best parameters\n",
    "    algo = SVD(n_factors=best_params['n_factors'],n_epochs=best_params['n_epochs'], lr_all=best_params['lr_all'], reg_all=best_params['reg_all'])\n",
    "    algo.fit(trainset)\n",
    "\n",
    "    # Save the trained model to a pickle file\n",
    "    with open('../model/trained_model.pkl', 'wb') as f:\n",
    "        pickle.dump(algo, f)\n",
    "    \n",
    "    # Predict ratings for the testset\n",
    "    predictions_test = algo.test(testset)\n",
    "\n",
    "    # Calculate precision and recall at k\n",
    "    precisions, recalls = precision_recall_at_k(predictions_test, k_list=[5, 10])\n",
    "    for k in [5, 10]:\n",
    "        avg_precision = sum(prec for prec in precisions[k].values()) / len(precisions[k])\n",
    "        avg_recall = sum(rec for rec in recalls[k].values()) / len(recalls[k])\n",
    "        print(f\"Average Precision at {k}: {avg_precision}\")\n",
    "        print(f\"Average Recall at {k}: {avg_recall}\")\n",
    "\n",
    "    # Get the list of articles read by the user\n",
    "    articles_read = clicks[clicks['user_id'] == user_id]['click_article_id'].tolist()\n",
    "\n",
    "    # Get the list of all articles\n",
    "    all_articles = list(articles.index)\n",
    "\n",
    "    # Remove the articles already read by the user\n",
    "    articles_to_predict = [article for article in all_articles if article not in articles_read]\n",
    "\n",
    "    # Get the predicted ratings for the articles not yet read by the user\n",
    "    predictions = {article: algo.predict(user_id, article).est for article in articles_to_predict}\n",
    "\n",
    "    # Get the top n articles\n",
    "    top_n_articles = nlargest(n, predictions, key=predictions.get)\n",
    "\n",
    "    return top_n_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision at 5: 0.37128164921462264\n",
      "Average Recall at 5: 0.9291109612466487\n",
      "Average Precision at 10: 0.38862830158889294\n",
      "Average Recall at 10: 0.9818411854381776\n"
     ]
    }
   ],
   "source": [
    "recommended_articles = collaborativeFilteringRecommendArticle(embeddings_df_pca, clicks_df, 7723, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended articles for user 7723:\n",
      "[237071, 363925, 68851, 38823, 69463, 96173, 284209, 294899, 73431, 74396]\n"
     ]
    }
   ],
   "source": [
    "print(\"Recommended articles for user 7723:\")\n",
    "print(recommended_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision at 5: 0.36876561207265307\n",
      "Average Recall at 5: 0.929712230405752\n",
      "Average Precision at 10: 0.3858123828232042\n",
      "Average Recall at 10: 0.9818147578353784\n"
     ]
    }
   ],
   "source": [
    "recommended_articles = collaborativeFilteringRecommendArticle(embeddings_df_pca, clicks_df, 20 , n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision at 5: 0.3722328751005706\n",
      "Average Recall at 5: 0.9289775817281876\n",
      "Average Precision at 10: 0.3895908904791733\n",
      "Average Recall at 10: 0.9817093854315823\n"
     ]
    }
   ],
   "source": [
    "recommended_articles = collaborativeFilteringRecommendArticle(embeddings_df_pca, clicks_df, 5890 , n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Hybrid based filtering\n",
    "\n",
    "Le **filtrage hybride** est une méthode de recommandation qui combine plusieurs approches de filtrage pour améliorer la précision et la diversité des recommandations. Les approches couramment combinées incluent le filtrage collaboratif basé sur les utilisateurs, le filtrage collaboratif basé sur les articles et le filtrage basé sur le contenu. En utilisant plusieurs sources d'information, les systèmes hybrides peuvent compenser les faiblesses des méthodes individuelles et offrir des recommandations plus robustes.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Hybrid based - User-based collaborative filtering and content-based filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Étapes du Code du Modèle Hybride\n",
    "\n",
    "1. **Préparation des Données** :\n",
    "    - Les colonnes `user_id` et `click_article_id` sont converties en type entier.\n",
    "    - Les articles lus par l'utilisateur cible sont identifiés.\n",
    "    - Les embeddings des articles lus par l'utilisateur sont obtenus.\n",
    "    - Les articles lus par l'utilisateur sont retirés de la liste des articles.\n",
    "\n",
    "2. **Filtrage Basé sur le Contenu** :\n",
    "    - La similarité cosinus entre les articles lus par l'utilisateur et les autres articles est calculée.\n",
    "    - Les articles les plus similaires aux articles lus par l'utilisateur sont recommandés.\n",
    "\n",
    "3. **Filtrage Collaboratif** :\n",
    "    - Les utilisateurs qui ont lu les mêmes articles que l'utilisateur cible sont identifiés.\n",
    "    - Les articles lus par ces utilisateurs similaires sont identifiés.\n",
    "    - Les articles déjà lus par l'utilisateur cible sont retirés de ces recommandations.\n",
    "\n",
    "4. **Combinaison des Résultats** :\n",
    "    - Les recommandations du filtrage basé sur le contenu et du filtrage collaboratif sont combinées.\n",
    "    - Si le nombre de recommandations est inférieur à n, les articles les plus populaires sont ajoutés aux recommandations jusqu'à atteindre n.\n",
    "    - Les n premières recommandations sont retournées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_recommend_articles(articles, clicks, user_id, n=5):\n",
    "    # Convert user_id and click_article_id to integer type\n",
    "    clicks['user_id'] = clicks['user_id'].astype(int)\n",
    "    clicks['click_article_id'] = clicks['click_article_id'].astype(int)\n",
    "    articles.index = articles.index.astype(int)\n",
    "    \n",
    "    # Get the articles read by the target user\n",
    "    articles_read = clicks[clicks['user_id'] == user_id]['click_article_id'].values\n",
    "    \n",
    "    # Get the embeddings of the articles read by the user\n",
    "    articles_read_embedding = articles.loc[articles_read]\n",
    "    \n",
    "    # Remove the articles read by the user from the list of articles\n",
    "    articles = articles.drop(articles_read)\n",
    "    \n",
    "    # Calculate the cosine similarity between the articles read by the user and the other articles\n",
    "    matrix = cosine_similarity(articles_read_embedding, articles)\n",
    "    \n",
    "    # Recommend the articles most similar to the articles read by the user\n",
    "    content_based_recommendations = [int(articles.index[np.argmax(row)]) for row in matrix]\n",
    "    \n",
    "    # Get the users who have read the same articles as the target user\n",
    "    similar_users = clicks[clicks['click_article_id'].isin(articles_read)]['user_id'].unique()\n",
    "    \n",
    "    # Get the articles read by the similar users\n",
    "    similar_users_articles = clicks[clicks['user_id'].isin(similar_users)]['click_article_id'].unique()\n",
    "    \n",
    "    # Remove the articles already read by the target user\n",
    "    collaborative_filtering_recommendations = [article for article in similar_users_articles if article not in articles_read]\n",
    "    \n",
    "    # Combine the recommendations from the content-based and collaborative filtering approaches\n",
    "    recommendations = content_based_recommendations + collaborative_filtering_recommendations\n",
    "    \n",
    "    # If there are not enough recommendations, add the most popular articles\n",
    "    if len(recommendations) < n:\n",
    "        most_popular_articles = clicks['click_article_id'].value_counts().index.tolist()\n",
    "        for article in most_popular_articles:\n",
    "            if article not in recommendations and article not in articles_read:\n",
    "                recommendations.append(article)\n",
    "            if len(recommendations) == n:\n",
    "                break\n",
    "    \n",
    "    return recommendations[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[193444, 194822, 100068, 69693, 34955]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hybrid_recommend_articles(embeddings_df_pca, clicks_df, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Hybrid based - Model-based collaborative filtering and content-based filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Étapes du Code hybrid model based\n",
    "\n",
    "1. **Préparation des Données** :\n",
    "    - Convertir les colonnes `user_id` et `click_article_id` en type entier.\n",
    "    - Vérifier si `user_id` est présent dans les données de clics.\n",
    "    - Créer un nouveau DataFrame comptant le nombre de fois qu'un utilisateur a cliqué sur un article.\n",
    "\n",
    "2. **Chargement et Division des Données** :\n",
    "    - Utiliser un sous-ensemble des données pour le filtrage collaboratif afin d'éviter les problèmes de mémoire.\n",
    "    - Créer un lecteur et un objet de données avec une échelle de notation basée sur le nombre de clics.\n",
    "    - Diviser les données en ensembles d'entraînement et de test.\n",
    "\n",
    "3. **Entraînement du Modèle** :\n",
    "    - Entraîner un modèle SVD avec les meilleurs paramètres obtenus.\n",
    "    - Prédire les notations pour l'ensemble de test.\n",
    "\n",
    "4. **Calcul des Précisions et Rappels à k** :\n",
    "    - Utiliser les fonctions de précision et de rappel pour évaluer les performances du modèle sur les valeurs k = 5 et k = 10.\n",
    "    - Calculer et afficher les précisions et rappels moyens pour ces valeurs de k.\n",
    "\n",
    "5. **Recommandation Basée sur le Filtrage Collaboratif** :\n",
    "    - Obtenir la liste des articles lus par l'utilisateur.\n",
    "    - Obtenir la liste de tous les articles disponibles.\n",
    "    - Exclure les articles déjà lus par l'utilisateur.\n",
    "    - Prédire les notations pour les articles non lus et obtenir les n meilleurs articles.\n",
    "\n",
    "6. **Filtrage Basé sur le Contenu** :\n",
    "    - Utiliser une fonction de recommandation basée sur le contenu (`recommend_articles`) pour obtenir les n meilleurs articles.\n",
    "\n",
    "7. **Combinaison des Résultats** :\n",
    "    - Combiner les résultats des filtres collaboratifs et basés sur le contenu pour obtenir les meilleures recommandations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybridRecommendArticle(articles, clicks, user_id, n=5):\n",
    "    # Convert user_id and click_article_id to integer type\n",
    "    clicks['user_id'] = clicks['user_id'].astype(int)\n",
    "    clicks['click_article_id'] = clicks['click_article_id'].astype(int)\n",
    "    articles.index = articles.index.astype(int)\n",
    "    \n",
    "    # Check if user_id is in clicks\n",
    "    if user_id not in clicks['user_id'].values:\n",
    "        return f\"Error: User ID {user_id} not found in clicks data.\"\n",
    "\n",
    "    # Create a new DataFrame that counts the number of times a user clicked on an article\n",
    "    click_counts = clicks.groupby(['user_id', 'click_article_id']).size().reset_index(name='click_count')\n",
    "\n",
    "    # Use a smaller subset of data for the collaborative filtering to avoid memory issues\n",
    "    data_subset = click_counts\n",
    "\n",
    "    # Create a reader and a data object\n",
    "    reader = Reader(rating_scale=(1, data_subset.click_count.max()))  # assuming a click count of at least 1\n",
    "    data = Dataset.load_from_df(data_subset, reader)\n",
    "\n",
    "    # Split the data into train and test sets\n",
    "    trainset, testset = train_test_split(data, test_size=0.2)\n",
    "\n",
    "    # Train a SVD model with the best parameters\n",
    "    algo = SVD(n_factors=best_params['n_factors'],n_epochs=best_params['n_epochs'], lr_all=best_params['lr_all'], reg_all=best_params['reg_all'])\n",
    "    algo.fit(trainset)\n",
    "\n",
    "    # Save the trained model\n",
    "    #with open(model_path + 'model.pkl', 'wb') as f:\n",
    "        #pickle.dump(algo, f)\n",
    "\n",
    "    # Predict ratings for the testset\n",
    "    predictions_test = algo.test(testset)\n",
    "\n",
    "    # Calculate precision and recall at k\n",
    "    precisions, recalls = precision_recall_at_k(predictions_test, k_list=[5, 10])\n",
    "    for k in [5, 10]:\n",
    "        avg_precision = sum(prec for prec in precisions[k].values()) / len(precisions[k])\n",
    "        avg_recall = sum(rec for rec in recalls[k].values()) / len(recalls[k])\n",
    "        print(f\"Average Precision at {k}: {avg_precision}\")\n",
    "        print(f\"Average Recall at {k}: {avg_recall}\")\n",
    "\n",
    "    # Get the list of articles read by the user\n",
    "    articles_read = clicks[clicks['user_id'] == user_id]['click_article_id'].tolist()\n",
    "\n",
    "    # Get the list of all articles\n",
    "    all_articles = list(articles.index)\n",
    "\n",
    "    # Remove the articles already read by the user\n",
    "    articles_to_predict = [article for article in all_articles if article not in articles_read]\n",
    "\n",
    "    # Get the predicted ratings for the articles not yet read by the user\n",
    "    predictions = {article: algo.predict(user_id, article).est for article in articles_to_predict}\n",
    "\n",
    "    # Get the top n articles\n",
    "    top_n_articles_collab = nlargest(n, predictions, key=predictions.get)\n",
    "\n",
    "    # Content-based filtering\n",
    "    top_n_articles_content = recommend_articles(articles, clicks, user_id, n)\n",
    "\n",
    "    # Combine the results of collaborative and content-based filtering\n",
    "    top_n_articles = top_n_articles_collab + top_n_articles_content\n",
    "\n",
    "    return top_n_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision at 5: 0.3687001003359955\n",
      "Average Recall at 5: 0.9292172292308205\n",
      "Average Precision at 10: 0.38611761542141\n",
      "Average Recall at 10: 0.9821256779209091\n",
      "Articles read by user 7723: [214455, 9308, 9649, 129799, 141548, 336220, 353673, 337192, 84763, 107179, 199197, 271400, 84835, 338339, 60252, 303565, 31520, 36685, 36609, 163505, 123434, 141050, 313504, 272660, 72618, 72646, 140445, 277491, 226648, 57740, 128551, 140324, 198659, 166581, 156560, 282964, 225124, 277491, 128707]\n",
      "Number of articles read by user 7723: 39\n",
      "Remaining articles after removing articles read by user 7723: 364009\n",
      "Number of recommended articles that have already been read by user 7723: 0\n"
     ]
    }
   ],
   "source": [
    "recommended_articles = hybridRecommendArticle(embeddings_df_pca, clicks_df, 7723, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision at 5: 0.375531967466616\n",
      "Average Recall at 5: 0.9286110447763688\n",
      "Average Precision at 10: 0.3929754116206176\n",
      "Average Recall at 10: 0.9818623142331041\n",
      "Articles read by user 20: [157541, 157485]\n",
      "Number of articles read by user 20: 2\n",
      "Remaining articles after removing articles read by user 20: 364045\n",
      "Number of recommended articles that have already been read by user 20: 0\n"
     ]
    }
   ],
   "source": [
    "recommended_articles = hybridRecommendArticle(embeddings_df_pca, clicks_df, 20, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision at 5: 0.3713205647566355\n",
      "Average Recall at 5: 0.9286830976257195\n",
      "Average Precision at 10: 0.3888101077521943\n",
      "Average Recall at 10: 0.98184394435023\n",
      "Articles read by user 5890: [207757, 21267, 264111, 144930, 66066, 111933, 2344, 236189, 3828, 132820, 142148, 66346, 111482, 198222, 283976, 114458, 207309, 288840, 202455, 206934, 59452, 194573, 207622, 65495, 65413, 288564, 183615, 283608, 9175, 206168, 206639, 47944, 206805, 217579, 162147, 140299, 119541, 146110, 225697, 96120, 194599, 145823, 96161, 177817, 235440, 206808, 120875, 36622, 65656, 14324, 270326, 59408, 65407, 206534, 29994, 133142, 76373, 208199, 16882, 47862, 206730, 61749, 119455, 96106, 95716, 96269, 166462, 233717, 199197, 84136, 66066, 198538, 207757, 21267, 145165, 29975, 65991, 9668, 198758, 59219, 57550, 261791, 264111, 57757, 118918, 225010, 144930, 285675, 261792, 183237, 111933, 208094, 242023, 57576, 58043, 235287, 59225, 206778, 58445, 360990, 293467, 244858, 76257, 58733, 136208, 97608, 224828, 234633, 2786, 14178, 324767, 338523, 207543, 59589, 242816, 118914, 304639, 207082, 21268, 263896, 224714, 58647, 293079, 141239, 91352, 19555, 113926, 288452, 124228, 123909, 233420, 207634, 237524, 203288, 203178, 202729, 202587, 123290, 58637, 114055, 96541, 202376, 73278, 58373, 123757, 119164, 207714, 293083, 206711, 225110, 336271, 298382, 70499, 195665, 283765, 237033, 208231, 14894, 206775, 119583, 177720, 235961, 258064, 342853, 202701, 10344, 208071, 183380, 208082, 321989, 3828, 205982, 48810, 320667, 203217, 66273, 118911, 19234, 38625, 203425, 71161, 85050, 202416, 202476, 225010, 19234, 7908, 71161, 202416, 202576, 208322, 203153, 141448, 206168, 175186, 182977, 8394, 272143, 64329, 166430, 156560, 96077, 288931, 265337, 235769, 271528, 219822, 144878, 48109, 177829, 209156, 59564, 307175, 58333, 203199, 30170, 71136, 207630, 69133, 260195, 136842, 292500, 362553, 83822, 96580, 285212, 119026, 66365, 245419, 208531, 199716, 304944, 264279, 324500, 118444, 284913, 235812, 100947, 300585, 53920, 53921, 207834, 206861, 58137, 208233, 327994, 288482, 360884, 207963, 173104, 207657, 79370, 207848, 208237, 59723, 288457, 31999, 202651, 68691, 208032, 58099, 119235, 298684, 203219, 255412, 304609, 58261, 119084, 206427, 202330, 198225, 330853, 48647, 198594, 331116, 289003, 73506, 57909, 58258, 218355, 144840, 202320, 198605, 60249, 315146, 58710, 60237, 59011, 255809, 60245, 72621, 72617, 72645, 78753, 220866, 220789, 307282, 57646, 290354, 290268, 283236, 68924, 69836, 198594, 107060, 107253, 107218, 106999, 107170, 208235, 97082, 312415, 325039, 136906, 15812, 57644, 360526, 59742, 233599, 4011, 283147, 207760, 2764, 199766, 283283, 234698, 114091, 206938, 205866, 207487, 203315, 202354, 59426, 59057, 309540, 114145, 209294, 65415, 285877, 70429, 113917, 203393, 313962, 57909, 84304, 136138, 68696, 118822, 272851, 114462, 183846, 96494, 58258, 202320, 59927, 260986, 299293, 198605, 47887, 207758, 289438, 206415, 245453, 343296, 200064, 61251, 224821, 177376, 118887, 145776, 118478, 353712, 285908, 265079, 144840, 218355, 71192, 271501, 233468, 234928, 57765, 284470, 235105, 206571, 184259, 57416, 207975, 292776, 303770, 304792, 140932, 95736, 208587, 66380, 285939, 205876, 331939, 207374, 207374, 331939, 195586, 195586, 207374, 124629, 183751, 236179, 248373, 233605, 331939, 277440, 57997, 360463, 202381, 283791, 202427, 208440, 203454, 207603, 307501, 181516, 21128, 85050, 66065, 75752, 352121, 78740, 96494, 307501, 157478, 352121, 65928, 61194, 61169, 60224, 96494, 206785, 157478, 206120, 207981, 63304, 202355, 57404, 205845, 209236, 202559, 202644, 203175, 207603, 57966, 58499, 59156, 58793, 1844, 119310, 363068, 195981, 315146, 149691, 48460, 283791, 207487, 237261, 202308, 203406, 202632, 58569, 57718, 68924, 63307, 16470, 62567, 288931, 265337, 235769, 333206, 121714, 100952, 277355, 235407, 203260, 255789, 141561, 209296, 361845, 21272, 308011, 31836, 195128, 118629, 284024, 10238, 121163, 184032, 15487, 361970, 285663, 59426, 57668, 83749, 208500, 225019, 289186, 57777, 207813, 59440, 119193, 217575, 199291, 299871, 4035, 58190, 338129, 324395, 195887, 224000, 58837, 145574, 225416, 32295, 145592, 181230, 121604, 57584, 136088, 292562, 285995, 189964, 202398, 133859, 235276, 124072, 289003, 118286, 97433, 123756, 129434, 255494, 91361, 233443, 206661, 202383, 96812, 205876, 58337, 234616, 202586, 30090, 236171, 65896, 145349, 61194, 235639, 124073, 71502, 236400, 207022, 213927, 308691, 114145, 285939, 2322, 326769, 209294, 273394, 20553, 119586, 100948, 282785, 207731, 225055, 206570, 65415, 49025, 177830, 70755, 65753, 47874, 314210, 59730, 16616, 133302, 182253, 303391, 84242, 9135, 121359, 207024, 321641, 203218, 59924, 206159, 59499, 300691, 289186, 119193, 205879, 203293, 202557, 206402, 206911, 203313, 289464, 13607, 59582, 58096, 65415, 353724, 106886, 205859, 312402, 198979, 182136, 314952, 95645, 220854, 220839, 220769, 72619, 72633, 290360, 62438, 283817, 48780, 48063, 235840, 106799, 106781, 167245, 106955, 106993, 106886, 300537, 106819, 106991, 59176, 95648, 207122, 169168, 209723, 324520, 270564, 59047, 57931, 202436, 57850, 293552, 242185, 303878, 119592, 181145, 304284, 205897, 285945, 234252, 236183, 58697, 58891, 20293, 198554, 198551, 70217, 206763, 209348, 208970, 208582, 208981, 209245, 202652, 107009, 119027, 220857, 220851, 290358, 95857, 107171, 107056, 107001, 183716, 208583, 19385, 58432, 95917, 58504, 205869, 202325, 20200, 273201, 176537, 260319, 59704, 59431, 68866, 195106, 353724, 235840, 207485, 206221, 7863, 206526, 207068, 284471, 207633, 203323, 202458, 235276, 285995, 208518, 225416, 207679, 202398, 59158, 57962, 58715, 58526, 58284, 273148, 303727, 298849, 32295, 146077, 42226, 284593, 30277, 87179, 214800, 206259, 207035, 202684, 208408, 207391, 203353, 202428, 202528, 57543, 58628, 59034, 58968, 59704, 8417, 191903, 62445, 236172, 15531, 321442, 284666, 5353, 261269, 331939, 208380, 202763, 208430, 235295, 207514, 207278, 203340, 57615, 70457, 59155, 198554, 265332, 60249, 58301, 70217, 107001, 205869, 96141, 194944, 119310, 4395, 59016, 146007, 202644, 111289, 78765, 78748, 3867, 75752, 242351, 169098, 332273, 273171, 360433, 163077, 288644, 57428, 15636, 60242, 59564, 58565, 288482, 118444, 66365, 360884, 207848, 207963, 308784, 271526, 58565, 208525, 57717, 203278, 202626, 202299, 114091, 303727, 273148, 30277, 64409, 59158, 288321, 235407, 129578, 328089, 308768, 19583, 309540, 194976, 328089, 308768, 19583, 309540, 194976, 206763, 208582, 237822, 107009, 290358, 202325, 70217, 209245, 265332, 285635, 203242, 202476, 202630, 202522, 58556, 58080, 58536, 58551, 205873, 352204, 260494, 111095, 308372, 173639, 250917, 303418, 338421, 71161, 172760, 58566, 172652, 182193, 142166, 19169, 100915, 283766, 100944, 225837, 293450, 270962, 205992, 146035, 65383, 207167, 122056, 57490, 328055, 206662, 96092, 174769, 145275, 48605, 15948, 175199, 235804, 303518, 8394, 182977, 58619, 235443, 203415, 203264, 58391, 58337, 49025, 273394, 177830, 70036, 208077, 205861, 207825, 234493, 206159, 207469, 207540, 225745, 206606, 203218, 202675, 321641, 255494, 114145, 233443, 263693, 58689, 57607, 58643, 59499, 91369, 107195, 174064, 184218, 208539, 202628, 202480, 59467, 203423, 83751, 59117, 57891, 266009, 57568, 68764, 71161, 157375, 58556, 58080, 233717, 3163, 288840, 114458, 194599, 14324, 65753, 70755, 2322, 58891, 58395, 49020, 220346, 284052, 321641, 59829, 174333, 129540, 39703, 121991, 68684, 208232, 240069, 289372, 21269, 225041, 272308, 237278, 65493, 58417, 30733, 69580, 58376, 205901, 237665, 207282, 207819, 119336, 4696, 205846, 58882, 286047, 208048, 58136, 135798, 361186, 120936, 195563, 175136, 2044, 224763, 202442, 286030, 14192, 208473, 58891, 209200, 119341, 119099, 145439, 217906, 88765, 29766, 15386, 234615, 16764, 164570, 47856, 158252, 220346, 284052, 272242, 336268, 58395, 198834, 293232, 141491, 299676, 208057, 177176, 100929, 145590, 362239, 49020, 207514, 57950, 235451, 198615, 59155, 20456, 133320, 84684, 207278, 224437, 65754, 217729, 124177, 123757, 244913, 122018, 352925, 114479, 48266, 288500, 260319, 209348, 208970, 167329, 70079, 206768, 58898, 206778, 9960, 59225, 57576, 58043, 206315, 225124, 206631, 208596, 205982, 202478, 207797, 203217, 202376, 58532, 57651, 58664, 58929, 58373, 236189, 175443, 194599, 8394, 177720, 352925, 244913, 114479, 58136, 343084, 66193, 176606, 198859, 202429, 234336, 263543, 65588, 208973, 144917, 65707, 283022, 237257, 303597, 236645, 199196, 213928, 207311, 224069, 181387, 83852, 69631, 202326, 69781, 202289, 190943, 235244, 261034, 218458, 205847, 191467, 58639, 233464, 70092, 133206, 32085, 202492, 206211, 36910, 57545, 272228, 218058, 353457, 58771, 48374, 118748, 208232, 68684, 53920, 240069, 30275, 305281, 172845, 240069, 208232, 208360, 68684, 208032, 207262, 207397, 203219, 202577, 202330, 30275, 195513, 118443, 119235, 59400, 59077, 58971, 58099, 173716, 198557, 114905, 208155, 206112, 206098, 206090, 207994, 206407, 207982, 208578, 206462, 202741, 203148, 202672, 203241, 206730, 206999, 207812, 206538, 206534, 208199, 57779, 58242, 58606, 59436, 58769, 59408, 58193, 172042, 30424, 194599, 14324, 10305, 96161, 65407, 326819, 236189, 57563, 3828, 132820, 57651, 205973, 203351, 57563, 202712, 59228, 59568, 63644, 237162, 57568, 263507, 69702, 145848, 361571, 29823, 220518, 272794, 57891, 57698, 118492, 202388, 57721, 59117, 351574, 224354, 75915, 118633, 202670, 236904, 207828, 360606, 273529, 208508, 233887, 118585, 172294, 206724, 211732, 58566, 172652, 68764, 361484, 19555, 59041, 17045, 9649, 273496, 141174, 61554, 324665, 235300, 203141]\n",
      "Number of articles read by user 5890: 1232\n",
      "Remaining articles after removing articles read by user 5890: 362999\n",
      "Number of recommended articles that have already been read by user 5890: 0\n"
     ]
    }
   ],
   "source": [
    "recommended_articles = hybridRecommendArticle(embeddings_df_pca, clicks_df, 5890, n=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projet10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
