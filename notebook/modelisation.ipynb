{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projet 10: Application de recommandation de contenu\n",
    "### Partie 2: Modèlisation \n",
    "Dans ce notebook on vas procéder à effectuer la modélisation des nos données pour mettre en place une systeme de recommandation du contenu.\n",
    "Nous allons utiliser deux approches principales:\n",
    "- Content-based filtering\n",
    "- Collaborative-based filtering\n",
    "\n",
    "Nous allons nous appuyer sur la libraries [Surpise](https://surpriselib.com/) pour mettre en place nos modèles "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Import des libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import surprise\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from math import floor\n",
    "import numpy as np\n",
    "\n",
    "from surprise import SVD, Dataset, Reader\n",
    "from surprise.model_selection import GridSearchCV\n",
    "\n",
    "from surprise import Reader, Dataset, KNNBasic, SVD\n",
    "from surprise.model_selection import train_test_split\n",
    "from heapq import nlargest\n",
    "from surprise import Reader, Dataset, KNNBasic, SVD\n",
    "from surprise.model_selection import train_test_split\n",
    "from heapq import nlargest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Import des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant nous allons procéder aves l'importations des libraries que nous allons utiliser pour la modèlisation et puis on vas importer les fichiers des données que nous allons utiliser comme base pour les modèles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 Définition des chemins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/raw/globocom/\"\n",
    "clicks_path= \"../data/raw/globocom/clicks/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 Import fichier avec metadonnées des articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>category_id</th>\n",
       "      <th>publisher_id</th>\n",
       "      <th>words_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>162</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id  category_id  publisher_id  words_count\n",
       "0           0            0             0          168\n",
       "1           1            1             0          189\n",
       "2           2            1             0          250\n",
       "3           3            1             0          230\n",
       "4           4            1             0          162"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_df = pd.read_csv(data_path + 'articles_metadata.csv')\n",
    "articles_df.drop(columns=['created_at_ts'], inplace=True)\n",
    "articles_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['article_id', 'category_id', 'publisher_id', 'words_count'], dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3 Import fichiers avec embeddings des articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>240</th>\n",
       "      <th>241</th>\n",
       "      <th>242</th>\n",
       "      <th>243</th>\n",
       "      <th>244</th>\n",
       "      <th>245</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.161183</td>\n",
       "      <td>-0.957233</td>\n",
       "      <td>-0.137944</td>\n",
       "      <td>0.050855</td>\n",
       "      <td>0.830055</td>\n",
       "      <td>0.901365</td>\n",
       "      <td>-0.335148</td>\n",
       "      <td>-0.559561</td>\n",
       "      <td>-0.500603</td>\n",
       "      <td>0.165183</td>\n",
       "      <td>...</td>\n",
       "      <td>0.321248</td>\n",
       "      <td>0.313999</td>\n",
       "      <td>0.636412</td>\n",
       "      <td>0.169179</td>\n",
       "      <td>0.540524</td>\n",
       "      <td>-0.813182</td>\n",
       "      <td>0.286870</td>\n",
       "      <td>-0.231686</td>\n",
       "      <td>0.597416</td>\n",
       "      <td>0.409623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.523216</td>\n",
       "      <td>-0.974058</td>\n",
       "      <td>0.738608</td>\n",
       "      <td>0.155234</td>\n",
       "      <td>0.626294</td>\n",
       "      <td>0.485297</td>\n",
       "      <td>-0.715657</td>\n",
       "      <td>-0.897996</td>\n",
       "      <td>-0.359747</td>\n",
       "      <td>0.398246</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.487843</td>\n",
       "      <td>0.823124</td>\n",
       "      <td>0.412688</td>\n",
       "      <td>-0.338654</td>\n",
       "      <td>0.320787</td>\n",
       "      <td>0.588643</td>\n",
       "      <td>-0.594137</td>\n",
       "      <td>0.182828</td>\n",
       "      <td>0.397090</td>\n",
       "      <td>-0.834364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.619619</td>\n",
       "      <td>-0.972960</td>\n",
       "      <td>-0.207360</td>\n",
       "      <td>-0.128861</td>\n",
       "      <td>0.044748</td>\n",
       "      <td>-0.387535</td>\n",
       "      <td>-0.730477</td>\n",
       "      <td>-0.066126</td>\n",
       "      <td>-0.754899</td>\n",
       "      <td>-0.242004</td>\n",
       "      <td>...</td>\n",
       "      <td>0.454756</td>\n",
       "      <td>0.473184</td>\n",
       "      <td>0.377866</td>\n",
       "      <td>-0.863887</td>\n",
       "      <td>-0.383365</td>\n",
       "      <td>0.137721</td>\n",
       "      <td>-0.810877</td>\n",
       "      <td>-0.447580</td>\n",
       "      <td>0.805932</td>\n",
       "      <td>-0.285284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.740843</td>\n",
       "      <td>-0.975749</td>\n",
       "      <td>0.391698</td>\n",
       "      <td>0.641738</td>\n",
       "      <td>-0.268645</td>\n",
       "      <td>0.191745</td>\n",
       "      <td>-0.825593</td>\n",
       "      <td>-0.710591</td>\n",
       "      <td>-0.040099</td>\n",
       "      <td>-0.110514</td>\n",
       "      <td>...</td>\n",
       "      <td>0.271535</td>\n",
       "      <td>0.036040</td>\n",
       "      <td>0.480029</td>\n",
       "      <td>-0.763173</td>\n",
       "      <td>0.022627</td>\n",
       "      <td>0.565165</td>\n",
       "      <td>-0.910286</td>\n",
       "      <td>-0.537838</td>\n",
       "      <td>0.243541</td>\n",
       "      <td>-0.885329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.279052</td>\n",
       "      <td>-0.972315</td>\n",
       "      <td>0.685374</td>\n",
       "      <td>0.113056</td>\n",
       "      <td>0.238315</td>\n",
       "      <td>0.271913</td>\n",
       "      <td>-0.568816</td>\n",
       "      <td>0.341194</td>\n",
       "      <td>-0.600554</td>\n",
       "      <td>-0.125644</td>\n",
       "      <td>...</td>\n",
       "      <td>0.238286</td>\n",
       "      <td>0.809268</td>\n",
       "      <td>0.427521</td>\n",
       "      <td>-0.615932</td>\n",
       "      <td>-0.503697</td>\n",
       "      <td>0.614450</td>\n",
       "      <td>-0.917760</td>\n",
       "      <td>-0.424061</td>\n",
       "      <td>0.185484</td>\n",
       "      <td>-0.580292</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 250 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0 -0.161183 -0.957233 -0.137944  0.050855  0.830055  0.901365 -0.335148   \n",
       "1 -0.523216 -0.974058  0.738608  0.155234  0.626294  0.485297 -0.715657   \n",
       "2 -0.619619 -0.972960 -0.207360 -0.128861  0.044748 -0.387535 -0.730477   \n",
       "3 -0.740843 -0.975749  0.391698  0.641738 -0.268645  0.191745 -0.825593   \n",
       "4 -0.279052 -0.972315  0.685374  0.113056  0.238315  0.271913 -0.568816   \n",
       "\n",
       "        7         8         9    ...       240       241       242       243  \\\n",
       "0 -0.559561 -0.500603  0.165183  ...  0.321248  0.313999  0.636412  0.169179   \n",
       "1 -0.897996 -0.359747  0.398246  ... -0.487843  0.823124  0.412688 -0.338654   \n",
       "2 -0.066126 -0.754899 -0.242004  ...  0.454756  0.473184  0.377866 -0.863887   \n",
       "3 -0.710591 -0.040099 -0.110514  ...  0.271535  0.036040  0.480029 -0.763173   \n",
       "4  0.341194 -0.600554 -0.125644  ...  0.238286  0.809268  0.427521 -0.615932   \n",
       "\n",
       "        244       245       246       247       248       249  \n",
       "0  0.540524 -0.813182  0.286870 -0.231686  0.597416  0.409623  \n",
       "1  0.320787  0.588643 -0.594137  0.182828  0.397090 -0.834364  \n",
       "2 -0.383365  0.137721 -0.810877 -0.447580  0.805932 -0.285284  \n",
       "3  0.022627  0.565165 -0.910286 -0.537838  0.243541 -0.885329  \n",
       "4 -0.503697  0.614450 -0.917760 -0.424061  0.185484 -0.580292  \n",
       "\n",
       "[5 rows x 250 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ouvrir le fichiers pickle et afficher les 5 premiers lignes\n",
    "with open(data_path + 'articles_embeddings.pickle', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "embeddings_df = pd.DataFrame(data)\n",
    "embeddings_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.4 Import fichier avec les interactions des utilisateurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_files_clicks(path):\n",
    "    clicks_df = pd.DataFrame()\n",
    "    for file in os.listdir(path):\n",
    "        df = pd.read_csv(path + file)\n",
    "        clicks_df = pd.concat([clicks_df, df], axis=0)\n",
    "\n",
    "    return clicks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "clicks_df = get_all_files_clicks(clicks_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>session_id</th>\n",
       "      <th>session_start</th>\n",
       "      <th>session_size</th>\n",
       "      <th>click_article_id</th>\n",
       "      <th>click_timestamp</th>\n",
       "      <th>click_environment</th>\n",
       "      <th>click_deviceGroup</th>\n",
       "      <th>click_os</th>\n",
       "      <th>click_country</th>\n",
       "      <th>click_region</th>\n",
       "      <th>click_referrer_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>93863</td>\n",
       "      <td>1507865792177843</td>\n",
       "      <td>2017-10-13 03:36:32</td>\n",
       "      <td>2</td>\n",
       "      <td>96210</td>\n",
       "      <td>2017-10-13 03:37:12.925</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>93863</td>\n",
       "      <td>1507865792177843</td>\n",
       "      <td>2017-10-13 03:36:32</td>\n",
       "      <td>2</td>\n",
       "      <td>158094</td>\n",
       "      <td>2017-10-13 03:37:42.925</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>294036</td>\n",
       "      <td>1507865795185844</td>\n",
       "      <td>2017-10-13 03:36:35</td>\n",
       "      <td>2</td>\n",
       "      <td>20691</td>\n",
       "      <td>2017-10-13 03:36:59.095</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>294036</td>\n",
       "      <td>1507865795185844</td>\n",
       "      <td>2017-10-13 03:36:35</td>\n",
       "      <td>2</td>\n",
       "      <td>96210</td>\n",
       "      <td>2017-10-13 03:37:29.095</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>77136</td>\n",
       "      <td>1507865796257845</td>\n",
       "      <td>2017-10-13 03:36:36</td>\n",
       "      <td>2</td>\n",
       "      <td>336245</td>\n",
       "      <td>2017-10-13 03:42:13.178</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_id        session_id       session_start session_size click_article_id  \\\n",
       "0   93863  1507865792177843 2017-10-13 03:36:32            2            96210   \n",
       "1   93863  1507865792177843 2017-10-13 03:36:32            2           158094   \n",
       "2  294036  1507865795185844 2017-10-13 03:36:35            2            20691   \n",
       "3  294036  1507865795185844 2017-10-13 03:36:35            2            96210   \n",
       "4   77136  1507865796257845 2017-10-13 03:36:36            2           336245   \n",
       "\n",
       "          click_timestamp click_environment click_deviceGroup click_os  \\\n",
       "0 2017-10-13 03:37:12.925                 4                 3        2   \n",
       "1 2017-10-13 03:37:42.925                 4                 3        2   \n",
       "2 2017-10-13 03:36:59.095                 4                 3       20   \n",
       "3 2017-10-13 03:37:29.095                 4                 3       20   \n",
       "4 2017-10-13 03:42:13.178                 4                 3        2   \n",
       "\n",
       "  click_country click_region click_referrer_type  \n",
       "0             1           21                   2  \n",
       "1             1           21                   2  \n",
       "2             1            9                   2  \n",
       "3             1            9                   2  \n",
       "4             1           25                   2  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clicks_df['click_timestamp'] = pd.to_datetime(clicks_df['click_timestamp'], unit='ms')\n",
    "clicks_df['session_start'] = pd.to_datetime(clicks_df['session_start'], unit='ms')\n",
    "clicks_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Content-based Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le **Content-based Filtering** est une méthode de recommandation qui utilise des informations détaillées sur les éléments pour recommander d'autres éléments similaires. Par exemple, dans un système de recommandation de films, le filtrage basé sur le contenu pourrait utiliser des informations telles que le genre du film, le réalisateur, les acteurs, etc.\n",
    "\n",
    "**Principe**\n",
    "L'idée est que si un utilisateur a aimé un certain élément dans le passé, il est probable qu'il aimera à nouveau des éléments similaires à l'avenir. Par conséquent, le système recommande des éléments qui sont similaires aux éléments que l'utilisateur a aimés précédemment.\n",
    "\n",
    "**Calcul de la similarité**\n",
    "La similarité entre les éléments est généralement calculée en utilisant des techniques telles que la similarité cosinus ou la distance euclidienne. Les éléments qui sont les plus similaires à ceux que l'utilisateur a aimés sont recommandés.\n",
    "\n",
    "**Note importante**\n",
    "Il est important de noter que le filtrage basé sur le contenu ne tient pas compte des opinions d'autres utilisateurs. Il se concentre uniquement sur les préférences de l'utilisateur actuel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici les étapes du code que nous allons utiliser: \n",
    "\n",
    "1. Nous allons d'abord identifier les articles que l'utilisateur a déjà lus. Cela se fait en filtrant le DataFrame clicks pour les lignes où le user_id correspond à l'utilisateur donné et en extrayant les valeurs de click_article_id.\n",
    "\n",
    "2. Si l'utilisateur n'a lu aucun article (c'est-à-dire que la liste des articles lus est vide), nous allons recommander les articles les plus populaires. La popularité est déterminée par le nombre de clics que chaque article a reçu, comme enregistré dans le DataFrame clicks.\n",
    "\n",
    "3. Si l'utilisateur a lu certains articles, nous allons obtenir les embeddings de ces articles à partir du DataFrame articles.\n",
    "\n",
    "4. Nous allons ensuite supprimer les articles que l'utilisateur a déjà lus de la liste de tous les articles.\n",
    "\n",
    "5. Nous allons calculer la similarité cosinus entre les embeddings des articles lus par l'utilisateur et les embeddings de tous les autres articles. Cela donne une matrice de similarité où chaque entrée représente la similarité entre un article lu par l'utilisateur et un autre article.\n",
    "\n",
    "6. Nous allons ensuite recommander les articles qui sont les plus similaires aux articles lus par l'utilisateur. Nous faisons cela en trouvant la valeur maximale dans la matrice de similarité, qui représente l'article le plus similaire. Les indices de cette valeur maximale sont utilisés pour trouver l'article correspondant dans le DataFrame articles.\n",
    "\n",
    "7. Nous allons répéter l'étape précédente n fois pour recommander n articles. Après chaque recommandation, nous mettons la similarité de l'article recommandé à 0 dans la matrice de similarité pour nous assurer que le même article n'est pas recommandé à nouveau.\n",
    "\n",
    "8. Enfin, nous allons retourner une liste des articles recommandés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contentBasedRecommendArticle(articles, clicks, user_id, n=5):\n",
    "    # Get the articles read by the user\n",
    "    articles_read = clicks[clicks['user_id'] == user_id]['click_article_id'].tolist()\n",
    "    print(f\"Articles read by user {user_id}: {articles_read}\")\n",
    "\n",
    "    # If the user hasn't read any articles, recommend the most popular ones\n",
    "    if len(articles_read) == 0:\n",
    "        most_popular_articles = clicks['click_article_id'].value_counts().index.tolist()\n",
    "        print(f\"User {user_id} has not read any articles. Recommending most popular articles: {most_popular_articles[:n]}\")\n",
    "        return most_popular_articles[:n]\n",
    "\n",
    "    # Get the embeddings of the articles read by the user\n",
    "    articles_read_embedding = articles.loc[articles_read]\n",
    "    print(f\"Embeddings of articles read by user {user_id}: {articles_read_embedding}\")\n",
    "\n",
    "    # Remove the articles read by the user from the list of articles\n",
    "    articles = articles.drop(articles_read)\n",
    "    print(f\"Remaining articles after removing articles read by user {user_id}: {articles}\")\n",
    "\n",
    "    # Calculate the cosine similarity between the articles read by the user and the other articles\n",
    "    matrix = cosine_similarity(articles_read_embedding, articles)\n",
    "    print(f\"Cosine similarity matrix: {matrix}\")\n",
    "\n",
    "    recommendations = []\n",
    "\n",
    "    # Recommend the articles most similar to the articles read by the user\n",
    "    for i in range(n):\n",
    "        coord_x = floor(np.argmax(matrix)/matrix.shape[1])\n",
    "        coord_y = np.argmax(matrix)%matrix.shape[1]\n",
    "\n",
    "        recommendations.append(int(articles.index[coord_y]))\n",
    "\n",
    "        # Set the similarity of the recommended article to 0\n",
    "        matrix[coord_x][coord_y] = 0\n",
    "\n",
    "    print(f\"Recommendations for user {user_id}: {recommendations}\")\n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contentBasedRecommendArticle(articles, clicks, user_id, n=5):\n",
    "    # Get the articles read by the user\n",
    "    articles_read = clicks[clicks['user_id'] == int(user_id)]['click_article_id'].tolist()\n",
    "    print(f\"Articles read by user {user_id}: {articles_read}\")\n",
    "\n",
    "    # If the user hasn't read any articles, recommend the most popular ones\n",
    "    if len(articles_read) == 0:\n",
    "        most_popular_articles = clicks['click_article_id'].value_counts().index.tolist()\n",
    "        print(f\"User {user_id} has not read any articles. Recommending most popular articles: {most_popular_articles[:n]}\")\n",
    "        return most_popular_articles[:n]\n",
    "\n",
    "    # Get the embeddings of the articles read by the user\n",
    "    articles_read_embedding = articles.loc[articles_read]\n",
    "    print(f\"Number of articles read by user {user_id}: {len(articles_read)}\")\n",
    "\n",
    "    # Remove the articles read by the user from the list of articles\n",
    "    articles = articles.drop(articles_read)\n",
    "    print(f\"Remaining articles after removing articles read by user {user_id}: {len(articles)}\")\n",
    "\n",
    "    # Calculate the cosine similarity between the articles read by the user and the other articles\n",
    "    matrix = cosine_similarity(articles_read_embedding, articles)\n",
    "\n",
    "    recommendations = []\n",
    "\n",
    "    # Recommend the articles most similar to the articles read by the user\n",
    "    for i in range(n):\n",
    "        coord_x = floor(np.argmax(matrix)/matrix.shape[1])\n",
    "        coord_y = np.argmax(matrix)%matrix.shape[1]\n",
    "\n",
    "        recommendations.append(int(articles.index[coord_y]))\n",
    "\n",
    "        # Set the similarity of the recommended article to 0\n",
    "        matrix[coord_x][coord_y] = 0\n",
    "\n",
    "\n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = \"77136\"\n",
    "n_recommendations = 10\n",
    "\n",
    "#recommended_articles = contentBasedRecommendArticle(embeddings_df, clicks_df, user_id, n_recommendations)\n",
    "\n",
    "#print(f\"Top {n_recommendations} recommended articles for user {user_id}: {recommended_articles}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Articles read by user 77136: [336245, 96210, 208150, 283505, 95972, 286108, 123909, 271262, 313920, 31836, 336223, 119193, 207603, 277104, 160974, 300470, 225463, 336221, 271261, 277133]\n",
      "Number of articles read by user 77136: 20\n",
      "Remaining articles after removing articles read by user 77136: 364027\n",
      "Precision@k: 0.0\n"
     ]
    }
   ],
   "source": [
    "def recommend_articles(articles, clicks, user_id, n=5):\n",
    "    # Convert user_id and click_article_id to integer type\n",
    "    clicks['user_id'] = clicks['user_id'].astype(int)\n",
    "    clicks['click_article_id'] = clicks['click_article_id'].astype(int)\n",
    "    articles.index = articles.index.astype(int)\n",
    "    \n",
    "    # Get the articles read by the user\n",
    "    articles_read = clicks[clicks['user_id'] == int(user_id)]['click_article_id'].tolist()\n",
    "    print(f\"Articles read by user {user_id}: {articles_read}\")\n",
    "\n",
    "    # If the user hasn't read any articles, recommend the most popular ones\n",
    "    if len(articles_read) == 0:\n",
    "        most_popular_articles = clicks['click_article_id'].value_counts().index.tolist()\n",
    "        print(f\"User {user_id} has not read any articles. Recommending most popular articles: {most_popular_articles[:n]}\")\n",
    "        return most_popular_articles[:n]\n",
    "\n",
    "    # Get the embeddings of the articles read by the user\n",
    "    articles_read_embedding = articles.loc[articles_read]\n",
    "    print(f\"Number of articles read by user {user_id}: {len(articles_read)}\")\n",
    "\n",
    "    # Remove the articles read by the user from the list of articles\n",
    "    articles = articles.drop(articles_read)\n",
    "    print(f\"Remaining articles after removing articles read by user {user_id}: {len(articles)}\")\n",
    "\n",
    "    # Calculate the cosine similarity between the articles read by the user and the other articles\n",
    "    matrix = cosine_similarity(articles_read_embedding, articles)\n",
    "\n",
    "    recommendations = []\n",
    "\n",
    "    # Recommend the articles most similar to the articles read by the user\n",
    "    for i in range(n):\n",
    "        coord_x = floor(np.argmax(matrix)/matrix.shape[1])\n",
    "        coord_y = np.argmax(matrix)%matrix.shape[1]\n",
    "\n",
    "        recommendations.append(int(articles.index[coord_y]))\n",
    "\n",
    "        # Set the similarity of the recommended article to 0\n",
    "        matrix[coord_x][coord_y] = 0\n",
    "\n",
    "    return recommendations\n",
    "\n",
    "def evaluate_recommendations(articles, clicks, user_id, n=5):\n",
    "    # Get the user's clicks\n",
    "    user_clicks = clicks[clicks['user_id'] == int(user_id)]['click_article_id'].tolist()\n",
    "\n",
    "    # Get the recommendations\n",
    "    recommendations = recommend_articles(articles, clicks, user_id, n)\n",
    "\n",
    "    # Calculate precision@k\n",
    "    relevant_recommendations = [rec for rec in recommendations if rec in user_clicks]\n",
    "    precision_at_k = len(relevant_recommendations) / n\n",
    "\n",
    "    return precision_at_k\n",
    "\n",
    "# Assuming `articles_df` is your articles data and `user_id` is the id of the user you want to evaluate\n",
    "precision_at_k = evaluate_recommendations(articles_df, clicks_df, user_id, 5)\n",
    "print(f\"Precision@k: {precision_at_k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User clicks: [336245, 96210, 208150, 283505, 95972, 286108, 123909, 271262, 313920, 31836, 336223, 119193, 207603, 277104, 160974, 300470, 225463, 336221, 271261, 277133]\n",
      "Articles read by user 77136: [336245, 96210, 208150, 283505, 95972, 286108, 123909, 271262, 313920, 31836, 336223, 119193, 207603, 277104, 160974, 300470, 225463, 336221, 271261, 277133]\n",
      "Similarity matrix shape: (20, 364047)\n",
      "Aggregated similarity scores: [0.00132258 0.00662165 0.00932843 ... 0.99999884 0.99999871 0.99999919]\n",
      "Recommended articles: [226028, 245736, 245549, 225736, 246041]\n",
      "Recommendations: [226028, 245736, 245549, 225736, 246041]\n",
      "Precision@k: 0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def recommend_articles(articles, clicks, user_id, n=5):\n",
    "    # Convert user_id and click_article_id to integer type\n",
    "    clicks['user_id'] = clicks['user_id'].astype(int)\n",
    "    clicks['click_article_id'] = clicks['click_article_id'].astype(int)\n",
    "    articles.index = articles.index.astype(int)\n",
    "    \n",
    "    # Get the articles read by the user\n",
    "    articles_read = clicks[clicks['user_id'] == int(user_id)]['click_article_id'].tolist()\n",
    "    print(f\"Articles read by user {user_id}: {articles_read}\")\n",
    "\n",
    "    # If the user hasn't read any articles, recommend the most popular ones\n",
    "    if len(articles_read) == 0:\n",
    "        most_popular_articles = clicks['click_article_id'].value_counts().index.tolist()\n",
    "        print(f\"User {user_id} has not read any articles. Recommending most popular articles: {most_popular_articles[:n]}\")\n",
    "        return most_popular_articles[:n]\n",
    "\n",
    "    # Get the embeddings of the articles read by the user\n",
    "    articles_read_embedding = articles.loc[articles_read].values\n",
    "\n",
    "    # Calculate the cosine similarity between the articles read by the user and all articles\n",
    "    similarity_matrix = cosine_similarity(articles_read_embedding, articles.values)\n",
    "    \n",
    "    print(f\"Similarity matrix shape: {similarity_matrix.shape}\")\n",
    "\n",
    "    # Aggregate similarity scores\n",
    "    similarity_scores = np.mean(similarity_matrix, axis=0)\n",
    "    \n",
    "    print(f\"Aggregated similarity scores: {similarity_scores}\")\n",
    "\n",
    "    # Get the indices of the top n articles, excluding those already read\n",
    "    recommendations = []\n",
    "    sorted_indices = np.argsort(similarity_scores)[::-1]\n",
    "    for idx in sorted_indices:\n",
    "        article_id = articles.index[idx]\n",
    "        if article_id not in articles_read:\n",
    "            recommendations.append(article_id)\n",
    "        if len(recommendations) == n:\n",
    "            break\n",
    "    \n",
    "    print(f\"Recommended articles: {recommendations}\")\n",
    "\n",
    "    return recommendations\n",
    "\n",
    "def evaluate_recommendations(articles, clicks, user_id, n=5):\n",
    "    # Get the user's clicks\n",
    "    user_clicks = clicks[clicks['user_id'] == int(user_id)]['click_article_id'].tolist()\n",
    "    print(f\"User clicks: {user_clicks}\")\n",
    "\n",
    "    # Get the recommendations\n",
    "    recommendations = recommend_articles(articles, clicks, user_id, n)\n",
    "    print(f\"Recommendations: {recommendations}\")\n",
    "\n",
    "    # Calculate precision@k\n",
    "    relevant_recommendations = [rec for rec in recommendations if rec in user_clicks]\n",
    "    precision_at_k = len(relevant_recommendations) / n\n",
    "\n",
    "    return precision_at_k\n",
    "\n",
    "# Example usage\n",
    "# Assuming `articles_df` is your articles data and `clicks_df` is your clicks data\n",
    "# and `user_id` is the id of the user you want to evaluate\n",
    "precision_at_k = evaluate_recommendations(articles_df, clicks_df, user_id, 5)\n",
    "print(f\"Precision@k: {precision_at_k}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User clicks: []\n",
      "User 77136 has not read any articles. Recommending most popular articles: [160974, 272143, 336221, 64329, 234698]\n",
      "Recommendations: [160974, 272143, 336221, 64329, 234698]\n",
      "Precision@k: 0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def build_user_item_matrix(clicks, max_users=1000, max_articles=1000):\n",
    "    # Limit the number of users and articles for testing\n",
    "    limited_clicks = clicks[clicks['user_id'].isin(clicks['user_id'].unique()[:max_users]) & clicks['click_article_id'].isin(clicks['click_article_id'].unique()[:max_articles])]\n",
    "    user_item_matrix = limited_clicks.pivot_table(index='user_id', columns='click_article_id', values='click_timestamp', aggfunc='count').fillna(0)\n",
    "    return csr_matrix(user_item_matrix.values), user_item_matrix.index, user_item_matrix.columns\n",
    "\n",
    "def recommend_articles(articles, clicks, user_id, n=5):\n",
    "    # Convert user_id and click_article_id to integer type\n",
    "    clicks['user_id'] = clicks['user_id'].astype(int)\n",
    "    clicks['click_article_id'] = clicks['click_article_id'].astype(int)\n",
    "    articles.index = articles.index.astype(int)\n",
    "\n",
    "    # Build user-item interaction matrix\n",
    "    user_item_matrix, user_index, item_index = build_user_item_matrix(clicks)\n",
    "    \n",
    "    if user_id not in user_index:\n",
    "        # If the user hasn't read any articles, recommend the most popular ones\n",
    "        most_popular_articles = clicks['click_article_id'].value_counts().index.tolist()\n",
    "        print(f\"User {user_id} has not read any articles. Recommending most popular articles: {most_popular_articles[:n]}\")\n",
    "        return most_popular_articles[:n]\n",
    "\n",
    "    # Find the index of the user\n",
    "    user_idx = user_index.get_loc(user_id)\n",
    "    \n",
    "    # Compute cosine similarity between users using sparse matrix\n",
    "    user_similarities = cosine_similarity(user_item_matrix[user_idx], user_item_matrix)\n",
    "    user_similarities_df = pd.DataFrame(user_similarities, index=user_index, columns=[user_id])\n",
    "    \n",
    "    # Get the users most similar to the target user\n",
    "    similar_users = user_similarities_df[user_id].sort_values(ascending=False).index.tolist()[1:]\n",
    "    print(f\"Users similar to user {user_id}: {similar_users[:5]}\")  # Print the top 5 similar users\n",
    "\n",
    "    # Aggregate the articles read by similar users, excluding those already read by the target user\n",
    "    articles_read_by_target_user = set(clicks[clicks['user_id'] == user_id]['click_article_id'])\n",
    "    recommended_articles = clicks[clicks['user_id'].isin(similar_users)]['click_article_id'].value_counts().index.tolist()\n",
    "    \n",
    "    # Filter out articles already read by the target user\n",
    "    recommended_articles = [article for article in recommended_articles if article not in articles_read_by_target_user]\n",
    "    \n",
    "    # Return the top n recommendations\n",
    "    print(f\"Recommended articles for user {user_id}: {recommended_articles[:n]}\")\n",
    "    return recommended_articles[:n]\n",
    "\n",
    "def evaluate_recommendations(articles, clicks, user_id, n=5):\n",
    "    # Get the user's clicks\n",
    "    user_clicks = clicks[clicks['user_id'] == int(user_id)]['click_article_id'].tolist()\n",
    "    print(f\"User clicks: {user_clicks}\")\n",
    "\n",
    "    # Get the recommendations\n",
    "    recommendations = recommend_articles(articles, clicks, user_id, n)\n",
    "    print(f\"Recommendations: {recommendations}\")\n",
    "\n",
    "    # Calculate precision@k\n",
    "    relevant_recommendations = [rec for rec in recommendations if rec in user_clicks]\n",
    "    precision_at_k = len(relevant_recommendations) / n\n",
    "\n",
    "    return precision_at_k\n",
    "\n",
    "# Example usage with reduced dataset\n",
    "# Assuming `articles_df` is your articles data and `clicks_df` is your clicks data\n",
    "# and `user_id` is the id of the user you want to evaluate\n",
    "\n",
    "# Reduce dataset for testing\n",
    "reduced_clicks_df = clicks_df.sample(n=10000, random_state=42)  # Adjust this number based on your dataset size\n",
    "reduced_articles_df = articles_df.loc[articles_df.index.isin(reduced_clicks_df['click_article_id'].unique())]\n",
    "\n",
    "try:\n",
    "    precision_at_k = evaluate_recommendations(reduced_articles_df, reduced_clicks_df, user_id, 5)\n",
    "    print(f\"Precision@k: {precision_at_k}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Collaborative-based Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le **Collaborative-based Filtering** est une méthode de recommandation qui se base sur les comportements passés des utilisateurs pour faire des prédictions sur ce qu'un utilisateur pourrait aimer.\n",
    "\n",
    "**Principe**\n",
    "L'idée principale est que si deux utilisateurs ont eu des comportements similaires par le passé (par exemple, ils ont aimé les mêmes films ou acheté les mêmes produits), alors ils sont susceptibles d'avoir des intérêts similaires à l'avenir.\n",
    "\n",
    "**Types de Filtrage Collaboratif**\n",
    "Il existe deux types principaux de filtrage collaboratif :\n",
    "\n",
    "1. **Filtrage Collaboratif Basé sur les Utilisateurs** : Cette méthode trouve des utilisateurs similaires à l'utilisateur cible et recommande des éléments que ces utilisateurs similaires ont aimés.\n",
    "\n",
    "2. **Filtrage Collaboratif Basé sur les Éléments** : Cette méthode trouve des éléments similaires à ceux que l'utilisateur cible a aimés et recommande ces éléments similaires.\n",
    "\n",
    "3. **Filtrage Collaboratif Basé sur un Modèle** : Cette méthode utilise des techniques de modélisation, comme la factorisation de matrices ou le clustering, pour prédire l'intérêt d'un utilisateur pour un élément. Elle se base sur les comportements passés de tous les utilisateurs, ainsi que sur les évaluations que l'utilisateur cible a données à d'autres éléments.\n",
    "\n",
    "**Calcul de la similarité**\n",
    "La similarité entre les utilisateurs ou les éléments est généralement calculée en utilisant des techniques telles que la corrélation de Pearson ou la similarité cosinus.\n",
    "\n",
    "**Note importante**\n",
    "Contrairement au filtrage basé sur le contenu, le filtrage collaboratif ne nécessite pas d'informations détaillées sur les éléments. Il se base uniquement sur les interactions passées entre les utilisateurs et les éléments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tout d'abord, nous allons rechercher les meilleurs paramètres pour le modèle en utilisant `GridSearchCV`. `GridSearchCV` est une méthode de recherche exhaustive qui parcourt toutes les combinaisons possibles de paramètres pour trouver celle qui produit le meilleur score de validation croisée.\n",
    "\n",
    "Dans le contexte de l'apprentissage automatique, les paramètres sont les configurations du modèle que nous ajustons pour améliorer la performance. Par exemple, dans un modèle de forêt aléatoire, les paramètres pourraient inclure le nombre d'arbres dans la forêt (`n_estimators`) et la profondeur maximale des arbres (`max_depth`).\n",
    "\n",
    "`GridSearchCV` fonctionne en entraînant et en évaluant un modèle pour chaque combinaison de paramètres. Il utilise la validation croisée pour évaluer la performance du modèle, ce qui signifie qu'il divise les données en un ensemble d'entraînement et un ensemble de test, entraîne le modèle sur l'ensemble d'entraînement, puis évalue la performance sur l'ensemble de test.\n",
    "\n",
    "Une fois que `GridSearchCV` a terminé la recherche, nous pouvons obtenir les meilleurs paramètres en utilisant l'attribut `best_params_`. Nous pouvons ensuite utiliser ces paramètres pour entraîner notre modèle final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'n_factors': 20, 'n_epochs': 20, 'lr_all': 0.005, 'reg_all': 0.04}\n"
     ]
    }
   ],
   "source": [
    "# Create a 'click_count' column\n",
    "clicks_df['click_count'] = clicks_df.groupby(['user_id', 'click_article_id'])['click_timestamp'].transform('count')\n",
    "\n",
    "# Load a fraction of the data into a Surprise dataset\n",
    "reader = Reader(rating_scale=(0, 1))\n",
    "data = Dataset.load_from_df(clicks_df[['user_id', 'click_article_id', 'click_count']].sample(frac=0.1, random_state=42), reader)\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_factors': [20, 50],\n",
    "    'n_epochs': [10, 20],\n",
    "    'lr_all': [0.002, 0.005],\n",
    "    'reg_all': [0.02, 0.04]\n",
    "}\n",
    "\n",
    "# Run a grid search with cross-validation\n",
    "gs = GridSearchCV(SVD, param_grid, measures=['rmse', 'mae'], cv=3)\n",
    "gs.fit(data)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = gs.best_params['rmse']\n",
    "\n",
    "print(f\"Best parameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best RMSE: 0.31750044531091054\n",
      "Best MAE: 0.03363276957546459\n"
     ]
    }
   ],
   "source": [
    "# Print the RMSE and MAE of the best model\n",
    "print(f\"Best RMSE: {gs.best_score['rmse']}\")\n",
    "print(f\"Best MAE: {gs.best_score['mae']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici les étapes du code que nous allons utiliser: \n",
    "\n",
    "Nous allons d'abord utiliser un sous-ensemble plus petit des données pour le filtrage collaboratif afin d'éviter les problèmes de mémoire. Nous faisons cela en échantillonnant une fraction des click_counts.\n",
    "\n",
    "Ensuite, nous allons créer un lecteur et un objet de données à partir de ce sous-ensemble de données.\n",
    "\n",
    "Nous allons diviser les données en ensembles d'entraînement et de test.\n",
    "\n",
    "Nous allons entraîner un modèle SVD avec les meilleurs paramètres.\n",
    "\n",
    "Nous allons obtenir la liste des articles lus par l'utilisateur. Nous faisons cela en filtrant les clicks pour les lignes où le user_id correspond à l'utilisateur donné et en extrayant les click_article_id.\n",
    "\n",
    "Nous allons obtenir la liste de tous les articles à partir de l'index du DataFrame articles.\n",
    "\n",
    "Nous allons supprimer les articles déjà lus par l'utilisateur de la liste de tous les articles.\n",
    "\n",
    "Nous allons obtenir les notes prédites pour les articles que l'utilisateur n'a pas encore lus. Nous faisons cela en utilisant le modèle SVD pour prédire la note de chaque article non lu.\n",
    "\n",
    "Nous allons obtenir les n meilleurs articles. Nous faisons cela en trouvant les n articles avec les notes prédites les plus élevées.\n",
    "\n",
    "Enfin, nous allons retourner la liste des n meilleurs articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collaborativeFilteringRecommendArticle(articles, clicks, user_id, n=5):\n",
    "    # Create a new DataFrame that counts the number of times a user clicked on an article\n",
    "    click_counts = clicks.groupby(['user_id', 'click_article_id']).size().reset_index(name='click_count')\n",
    "\n",
    "    # Use a smaller subset of data for the collaborative filtering to avoid memory issues\n",
    "    data_subset = click_counts.sample(frac=0.1, random_state=1)  # adjust the fraction as needed\n",
    "\n",
    "    # Create a reader and a data object\n",
    "    reader = Reader(rating_scale=(1, data_subset.click_count.max()))  # assuming a click count of at least 1\n",
    "    data = Dataset.load_from_df(data_subset, reader)\n",
    "\n",
    "    # Split the data into train and test sets\n",
    "    trainset, testset = train_test_split(data, test_size=0.2)\n",
    "\n",
    "\n",
    "    # Train a SVD model with the best parameters\n",
    "    algo = SVD(n_factors=best_params['n_factors'], n_epochs=best_params['n_epochs'], lr_all=best_params['lr_all'], reg_all=best_params['reg_all'])\n",
    "    algo.fit(trainset)\n",
    "\n",
    "    \n",
    "\n",
    "    # Get the list of articles read by the user\n",
    "    articles_read = clicks[clicks['user_id'] == user_id]['click_article_id'].tolist()\n",
    "\n",
    "    # Get the list of all articles\n",
    "    all_articles = list(articles.index)\n",
    "\n",
    "    # Remove the articles already read by the user\n",
    "    articles_to_predict = [article for article in all_articles if article not in articles_read]\n",
    "\n",
    "    # Get the predicted ratings for the articles not yet read by the user\n",
    "    predictions = {article: algo.predict(user_id, article).est for article in articles_to_predict}\n",
    "\n",
    "    # Get the top n articles\n",
    "    top_n_articles = nlargest(n, predictions, key=predictions.get)\n",
    "\n",
    "    return top_n_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 recommended articles for user 20: [225471, 76268, 32730, 298099, 158244]\n"
     ]
    }
   ],
   "source": [
    "user_id = '20'\n",
    "n_recommendations = 5\n",
    "\n",
    "recommended_articles = collaborativeFilteringRecommendArticle(articles_df, clicks_df, user_id, n_recommendations)\n",
    "\n",
    "print(f\"Top {n_recommendations} recommended articles for user {user_id}: {recommended_articles}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projet10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
